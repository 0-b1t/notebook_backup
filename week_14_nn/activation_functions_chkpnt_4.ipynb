{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib\\nimport tensorflow as tf\\n\\ntf.debugging.set_log_device_placement(True)\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.utils import to_categorical\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib\\nimport tensorflow as tf\\n\\ntf.debugging.set_log_device_placement(True)\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.utils import to_categorical\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "[[ 8.  5.]\n",
      " [20. 13.]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import os\\n\\ntf.compat.v1.disable_eager_execution()\\n\\nhello = tf.constant(\\\"Hello, TensorFlow!\\\")\\n\\n# sess = tf.compat.v1.Session()\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"  # You need to tell CUDA\\n# which GPU you'd like to use. if you have one GPU probably your GPU is '0'with tf.device('/gpu:0'):\\na = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name=\\\"a\\\")\\nb = tf.constant([4.0, 3.0, 2.0, 1.0], shape=[2, 2], name=\\\"b\\\")\\nc = tf.matmul(a, b)\\n# with tf.compat.v1.Session() as sess:\\n#     print(sess.run(c))\\n\\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\\n# Runs the op.\\nprint(sess.run(c))\";\n",
       "                var nbb_formatted_code = \"import os\\n\\ntf.compat.v1.disable_eager_execution()\\n\\nhello = tf.constant(\\\"Hello, TensorFlow!\\\")\\n\\n# sess = tf.compat.v1.Session()\\n\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0\\\"  # You need to tell CUDA\\n# which GPU you'd like to use. if you have one GPU probably your GPU is '0'with tf.device('/gpu:0'):\\na = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name=\\\"a\\\")\\nb = tf.constant([4.0, 3.0, 2.0, 1.0], shape=[2, 2], name=\\\"b\\\")\\nc = tf.matmul(a, b)\\n# with tf.compat.v1.Session() as sess:\\n#     print(sess.run(c))\\n\\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\\n# Runs the op.\\nprint(sess.run(c))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "\n",
    "# sess = tf.compat.v1.Session()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # You need to tell CUDA\n",
    "# which GPU you'd like to use. if you have one GPU probably your GPU is '0'with tf.device('/gpu:0'):\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name=\"a\")\n",
    "b = tf.constant([4.0, 3.0, 2.0, 1.0], shape=[2, 2], name=\"b\")\n",
    "c = tf.matmul(a, b)\n",
    "# with tf.compat.v1.Session() as sess:\n",
    "#     print(sess.run(c))\n",
    "\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"def build_model(\\n    input_dim,\\n    output_dim=1,\\n    layer_nodes=[64, 32, 32],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"sigmoid\\\",\\n):\\n    model = Sequential()\\n    model.add(\\n        Dense(layer_nodes[0], input_dim=input_dim, activation=activation_function)\\n    )\\n    for i in range(1, len(layer_nodes)):\\n        model.add(Dense(layer_nodes[i], activation=activation_function))\\n\\n    model.add(Dense(output_dim, activation=output_function))\\n\\n    return model\";\n",
       "                var nbb_formatted_code = \"def build_model(\\n    input_dim,\\n    output_dim=1,\\n    layer_nodes=[64, 32, 32],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"sigmoid\\\",\\n):\\n    model = Sequential()\\n    model.add(\\n        Dense(layer_nodes[0], input_dim=input_dim, activation=activation_function)\\n    )\\n    for i in range(1, len(layer_nodes)):\\n        model.add(Dense(layer_nodes[i], activation=activation_function))\\n\\n    model.add(Dense(output_dim, activation=output_function))\\n\\n    return model\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(\n",
    "    input_dim,\n",
    "    output_dim=1,\n",
    "    layer_nodes=[64, 32, 32],\n",
    "    activation_function=\"relu\",\n",
    "    output_function=\"sigmoid\",\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Dense(layer_nodes[0], input_dim=input_dim, activation=activation_function)\n",
    "    )\n",
    "    for i in range(1, len(layer_nodes)):\n",
    "        model.add(Dense(layer_nodes[i], activation=activation_function))\n",
    "\n",
    "    model.add(Dense(output_dim, activation=output_function))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\\ninput_dim = 784  # 28*28\\noutput_dim = nb_classes = 10\\nbatch_size = 128\\nnb_epoch = 20\\n\\nX_train = X_train.reshape(60000, input_dim)\\nX_test = X_test.reshape(10000, input_dim)\\nX_train = X_train.astype(\\\"float32\\\")\\nX_test = X_test.astype(\\\"float32\\\")\\nX_train /= 255\\nX_test /= 255\\n\\n\\ny_train = to_categorical(y_train, nb_classes)\\ny_test = to_categorical(y_test, nb_classes)\";\n",
       "                var nbb_formatted_code = \"(X_train, y_train), (X_test, y_test) = mnist.load_data()\\n\\ninput_dim = 784  # 28*28\\noutput_dim = nb_classes = 10\\nbatch_size = 128\\nnb_epoch = 20\\n\\nX_train = X_train.reshape(60000, input_dim)\\nX_test = X_test.reshape(10000, input_dim)\\nX_train = X_train.astype(\\\"float32\\\")\\nX_test = X_test.astype(\\\"float32\\\")\\nX_train /= 255\\nX_test /= 255\\n\\n\\ny_train = to_categorical(y_train, nb_classes)\\ny_test = to_categorical(y_test, nb_classes)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "y_train = to_categorical(y_train, nb_classes)\n",
    "y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\b1t\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "60000/60000 - 1s - loss: 0.3457 - accuracy: 0.9017 - val_loss: 0.1980 - val_accuracy: 0.9419\n",
      "Epoch 2/20\n",
      "60000/60000 - 1s - loss: 0.1622 - accuracy: 0.9522 - val_loss: 0.1430 - val_accuracy: 0.9566\n",
      "Epoch 3/20\n",
      "60000/60000 - 1s - loss: 0.1160 - accuracy: 0.9658 - val_loss: 0.1130 - val_accuracy: 0.9664\n",
      "Epoch 4/20\n",
      "60000/60000 - 0s - loss: 0.0864 - accuracy: 0.9750 - val_loss: 0.0978 - val_accuracy: 0.9676\n",
      "Epoch 5/20\n",
      "60000/60000 - 0s - loss: 0.0678 - accuracy: 0.9796 - val_loss: 0.0910 - val_accuracy: 0.9712\n",
      "Epoch 6/20\n",
      "60000/60000 - 0s - loss: 0.0531 - accuracy: 0.9843 - val_loss: 0.0896 - val_accuracy: 0.9716\n",
      "Epoch 7/20\n",
      "60000/60000 - 0s - loss: 0.0431 - accuracy: 0.9873 - val_loss: 0.0813 - val_accuracy: 0.9731\n",
      "Epoch 8/20\n",
      "60000/60000 - 1s - loss: 0.0334 - accuracy: 0.9904 - val_loss: 0.0817 - val_accuracy: 0.9731\n",
      "Epoch 9/20\n",
      "60000/60000 - 0s - loss: 0.0275 - accuracy: 0.9926 - val_loss: 0.0840 - val_accuracy: 0.9734\n",
      "Epoch 10/20\n",
      "60000/60000 - 1s - loss: 0.0211 - accuracy: 0.9947 - val_loss: 0.0745 - val_accuracy: 0.9777\n",
      "Epoch 11/20\n",
      "60000/60000 - 0s - loss: 0.0164 - accuracy: 0.9961 - val_loss: 0.0826 - val_accuracy: 0.9765\n",
      "Epoch 12/20\n",
      "60000/60000 - 0s - loss: 0.0132 - accuracy: 0.9969 - val_loss: 0.0796 - val_accuracy: 0.9781\n",
      "Epoch 13/20\n",
      "60000/60000 - 1s - loss: 0.0110 - accuracy: 0.9976 - val_loss: 0.0845 - val_accuracy: 0.9761\n",
      "Epoch 14/20\n",
      "60000/60000 - 1s - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.0817 - val_accuracy: 0.9769\n",
      "Epoch 15/20\n",
      "60000/60000 - 1s - loss: 0.0090 - accuracy: 0.9977 - val_loss: 0.0814 - val_accuracy: 0.9776\n",
      "Epoch 16/20\n",
      "60000/60000 - 1s - loss: 0.0050 - accuracy: 0.9992 - val_loss: 0.0831 - val_accuracy: 0.9786\n",
      "Epoch 17/20\n",
      "60000/60000 - 0s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.0970 - val_accuracy: 0.9751\n",
      "Epoch 18/20\n",
      "60000/60000 - 1s - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.0848 - val_accuracy: 0.9767\n",
      "Epoch 19/20\n",
      "60000/60000 - 1s - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.0993 - val_accuracy: 0.9746\n",
      "Epoch 20/20\n",
      "60000/60000 - 1s - loss: 0.0043 - accuracy: 0.9990 - val_loss: 0.1027 - val_accuracy: 0.9737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec0f18dc10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=20, verbose=2)\";\n",
       "                var nbb_formatted_code = \"model.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=2,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"tanh\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "55040/60000 [==========================>...] - ETA: 0s - loss: 0.8481 - accuracy: 0.8110WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "60000/60000 [==============================] - 1s 17us/sample - loss: 0.8063 - accuracy: 0.8191 - val_loss: 0.3222 - val_accuracy: 0.9157\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2729 - accuracy: 0.9244 - val_loss: 0.2245 - val_accuracy: 0.9358\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2016 - accuracy: 0.9427 - val_loss: 0.1783 - val_accuracy: 0.9487\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1597 - accuracy: 0.9542 - val_loss: 0.1473 - val_accuracy: 0.9549\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1306 - accuracy: 0.9626 - val_loss: 0.1299 - val_accuracy: 0.9596\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.1089 - accuracy: 0.9689 - val_loss: 0.1132 - val_accuracy: 0.9669\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0929 - accuracy: 0.9736 - val_loss: 0.1043 - val_accuracy: 0.9678\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0792 - accuracy: 0.9776 - val_loss: 0.0906 - val_accuracy: 0.9732\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0683 - accuracy: 0.9808 - val_loss: 0.0862 - val_accuracy: 0.9741\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0589 - accuracy: 0.9835 - val_loss: 0.0829 - val_accuracy: 0.9760\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0510 - accuracy: 0.9861 - val_loss: 0.0793 - val_accuracy: 0.9749\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0443 - accuracy: 0.9880 - val_loss: 0.0747 - val_accuracy: 0.9765\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0383 - accuracy: 0.9898 - val_loss: 0.0727 - val_accuracy: 0.9779\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0329 - accuracy: 0.9916 - val_loss: 0.0744 - val_accuracy: 0.9776\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0284 - accuracy: 0.9931 - val_loss: 0.0695 - val_accuracy: 0.9773\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0246 - accuracy: 0.9943 - val_loss: 0.0746 - val_accuracy: 0.9774\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0213 - accuracy: 0.9950 - val_loss: 0.0708 - val_accuracy: 0.9788\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0182 - accuracy: 0.9965 - val_loss: 0.0716 - val_accuracy: 0.9783\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0156 - accuracy: 0.9970 - val_loss: 0.0699 - val_accuracy: 0.9798\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0130 - accuracy: 0.9978 - val_loss: 0.0727 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec0f2269a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"sigmoid\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=20, verbose=1)\";\n",
       "                var nbb_formatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"sigmoid\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"sigmoid\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3344 - accuracy: 0.9066 - val_loss: 0.1564 - val_accuracy: 0.9513\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1361 - accuracy: 0.9599 - val_loss: 0.1089 - val_accuracy: 0.9666\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0957 - accuracy: 0.9715 - val_loss: 0.0934 - val_accuracy: 0.9701\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0723 - accuracy: 0.9784 - val_loss: 0.0872 - val_accuracy: 0.9718\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0583 - accuracy: 0.9824 - val_loss: 0.0865 - val_accuracy: 0.9725\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0466 - accuracy: 0.9860 - val_loss: 0.0864 - val_accuracy: 0.9712\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0364 - accuracy: 0.9892 - val_loss: 0.0810 - val_accuracy: 0.9762\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.0732 - val_accuracy: 0.9791\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0246 - accuracy: 0.9926 - val_loss: 0.0799 - val_accuracy: 0.9768\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.0835 - val_accuracy: 0.9763\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0176 - accuracy: 0.9947 - val_loss: 0.0774 - val_accuracy: 0.9787\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0855 - val_accuracy: 0.9773\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0908 - val_accuracy: 0.9755\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.0980 - val_accuracy: 0.9761\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0919 - val_accuracy: 0.9774\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0097 - accuracy: 0.9969 - val_loss: 0.0850 - val_accuracy: 0.9775\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0103 - accuracy: 0.9966 - val_loss: 0.0933 - val_accuracy: 0.9782\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0901 - val_accuracy: 0.9780\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0878 - val_accuracy: 0.9793\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0049 - accuracy: 0.9986 - val_loss: 0.1165 - val_accuracy: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec0f2ca760>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=20, verbose=1)\";\n",
       "                var nbb_formatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_crossentropy\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"relu\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're all almost identical but it seems sigmoid was the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 24us/sample - loss: 0.2641 - accuracy: 0.8882 - val_loss: 0.1470 - val_accuracy: 0.9355\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1276 - accuracy: 0.9440 - val_loss: 0.1116 - val_accuracy: 0.9496\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0956 - accuracy: 0.9571 - val_loss: 0.0896 - val_accuracy: 0.9590\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0759 - accuracy: 0.9662 - val_loss: 0.0804 - val_accuracy: 0.9637\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0631 - accuracy: 0.9720 - val_loss: 0.0690 - val_accuracy: 0.9688\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0525 - accuracy: 0.9770 - val_loss: 0.0631 - val_accuracy: 0.9707\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0465 - accuracy: 0.9793 - val_loss: 0.0588 - val_accuracy: 0.9732\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0407 - accuracy: 0.9820 - val_loss: 0.0615 - val_accuracy: 0.9711\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0354 - accuracy: 0.9846 - val_loss: 0.0565 - val_accuracy: 0.9736\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0321 - accuracy: 0.9860 - val_loss: 0.0536 - val_accuracy: 0.9748\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0288 - accuracy: 0.9875 - val_loss: 0.0533 - val_accuracy: 0.9743\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0263 - accuracy: 0.9886 - val_loss: 0.0540 - val_accuracy: 0.9741\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0240 - accuracy: 0.9895 - val_loss: 0.0552 - val_accuracy: 0.9740\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0215 - accuracy: 0.9905 - val_loss: 0.0481 - val_accuracy: 0.9768\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0197 - accuracy: 0.9913 - val_loss: 0.0496 - val_accuracy: 0.9764\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0187 - accuracy: 0.9919 - val_loss: 0.0511 - val_accuracy: 0.9754\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0168 - accuracy: 0.9924 - val_loss: 0.0464 - val_accuracy: 0.9780\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 9us/sample - loss: 0.0178 - accuracy: 0.9921 - val_loss: 0.0491 - val_accuracy: 0.9769\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0153 - accuracy: 0.9932 - val_loss: 0.0472 - val_accuracy: 0.9773\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0155 - accuracy: 0.9932 - val_loss: 0.0498 - val_accuracy: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec0f3dca00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"tanh\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=20, verbose=1)\";\n",
       "                var nbb_formatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"tanh\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"tanh\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_hinge\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 28us/sample - loss: 0.6645 - accuracy: 0.7574 - val_loss: 0.2613 - val_accuracy: 0.9077\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.2065 - accuracy: 0.9158 - val_loss: 0.1665 - val_accuracy: 0.9277\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1506 - accuracy: 0.9342 - val_loss: 0.1350 - val_accuracy: 0.9403\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1241 - accuracy: 0.9453 - val_loss: 0.1161 - val_accuracy: 0.9471\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1059 - accuracy: 0.9531 - val_loss: 0.1070 - val_accuracy: 0.9510\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0927 - accuracy: 0.9588 - val_loss: 0.0968 - val_accuracy: 0.9556\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0816 - accuracy: 0.9643 - val_loss: 0.0876 - val_accuracy: 0.9605\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0727 - accuracy: 0.9677 - val_loss: 0.0826 - val_accuracy: 0.9619\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0650 - accuracy: 0.9713 - val_loss: 0.0754 - val_accuracy: 0.9665\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0589 - accuracy: 0.9744 - val_loss: 0.0712 - val_accuracy: 0.9673\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0534 - accuracy: 0.9767 - val_loss: 0.0672 - val_accuracy: 0.9695\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0488 - accuracy: 0.9787 - val_loss: 0.0675 - val_accuracy: 0.9692\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0449 - accuracy: 0.9808 - val_loss: 0.0622 - val_accuracy: 0.9724\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0407 - accuracy: 0.9824 - val_loss: 0.0598 - val_accuracy: 0.9725\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0373 - accuracy: 0.9838 - val_loss: 0.0578 - val_accuracy: 0.9739\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.0347 - accuracy: 0.9847 - val_loss: 0.0563 - val_accuracy: 0.9743\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.0321 - accuracy: 0.9860 - val_loss: 0.0555 - val_accuracy: 0.9744\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0302 - accuracy: 0.9867 - val_loss: 0.0534 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0287 - accuracy: 0.9872 - val_loss: 0.0542 - val_accuracy: 0.9748\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0267 - accuracy: 0.9880 - val_loss: 0.0518 - val_accuracy: 0.9761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec0f4a8190>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"sigmoid\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_formatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"sigmoid\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"sigmoid\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_hinge\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.2462 - accuracy: 0.8931 - val_loss: 0.1192 - val_accuracy: 0.9468\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.1076 - accuracy: 0.9508 - val_loss: 0.0895 - val_accuracy: 0.9604\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0807 - accuracy: 0.9630 - val_loss: 0.0760 - val_accuracy: 0.9650\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0683 - accuracy: 0.9686 - val_loss: 0.0685 - val_accuracy: 0.9681\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 11us/sample - loss: 0.0565 - accuracy: 0.9742 - val_loss: 0.0568 - val_accuracy: 0.9735\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0505 - accuracy: 0.9766 - val_loss: 0.0607 - val_accuracy: 0.9717\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0446 - accuracy: 0.9794 - val_loss: 0.0573 - val_accuracy: 0.9723\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0407 - accuracy: 0.9812 - val_loss: 0.0556 - val_accuracy: 0.9739\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0369 - accuracy: 0.9830 - val_loss: 0.0554 - val_accuracy: 0.9725\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0346 - accuracy: 0.9838 - val_loss: 0.0548 - val_accuracy: 0.9738\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0327 - accuracy: 0.9846 - val_loss: 0.0556 - val_accuracy: 0.9733\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0300 - accuracy: 0.9860 - val_loss: 0.0520 - val_accuracy: 0.9748\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0277 - accuracy: 0.9869 - val_loss: 0.0466 - val_accuracy: 0.9774\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0270 - accuracy: 0.9874 - val_loss: 0.0485 - val_accuracy: 0.9764\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0249 - accuracy: 0.9882 - val_loss: 0.0497 - val_accuracy: 0.9757\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.0235 - accuracy: 0.9889 - val_loss: 0.0516 - val_accuracy: 0.9739\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0221 - accuracy: 0.9897 - val_loss: 0.0505 - val_accuracy: 0.9746\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0213 - accuracy: 0.9900 - val_loss: 0.0478 - val_accuracy: 0.9764\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0205 - accuracy: 0.9904 - val_loss: 0.0529 - val_accuracy: 0.9743\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 10us/sample - loss: 0.0202 - accuracy: 0.9905 - val_loss: 0.0463 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ec1e8a3280>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_formatted_code = \"model = build_model(\\n    input_dim=X_train.shape[1],\\n    output_dim=y_train.shape[1],\\n    layer_nodes=[128, 64],\\n    activation_function=\\\"relu\\\",\\n    output_function=\\\"softmax\\\",\\n)\\n\\nmodel.compile(optimizer=\\\"adam\\\", loss=\\\"categorical_hinge\\\", metrics=[\\\"accuracy\\\"])\\n\\nmodel.fit(\\n    X_train,\\n    y_train,\\n    validation_data=(X_test, y_test),\\n    batch_size=128,\\n    epochs=20,\\n    verbose=1,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=y_train.shape[1],\n",
    "    layer_nodes=[128, 64],\n",
    "    activation_function=\"relu\",\n",
    "    output_function=\"softmax\",\n",
    ")\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_hinge\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=128,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like relu is the best overall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
