{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKSZ10Al0lsa"
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "In this assignment, we will learn about loss functions. We will use a create a neural network and measure the model's performance using different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uC8ZyHPq0lsb",
    "outputId": "768dce31-23c6-45eb-c96d-c1b74b553775"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n\\ntf.debugging.set_log_device_placement(True)\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\npd.set_option(\\\"display.max_rows\\\", 100)\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\n\\ntf.debugging.set_log_device_placement(True)\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense\\n\\nimport warnings\\n\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\npd.set_option(\\\"display.max_rows\\\", 100)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqKRN1_O0lse"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"housing = pd.read_csv(\\\"data/housing.csv\\\")\";\n",
       "                var nbb_formatted_code = \"housing = pd.read_csv(\\\"data/housing.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"data/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5WgUX-I0lsh",
    "outputId": "ee98131f-1192-40fe-a7e3-8353628e3dc2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"housing.head()\";\n",
       "                var nbb_formatted_code = \"housing.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYgIVabm0lsj"
   },
   "source": [
    "We will use the dataset above to predict housing prices using various features about each house. Our first step is to check for missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_gFN2cu0lsj"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\n\\nmissing = housing.isna().mean()\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\n\\nmissing = housing.isna().mean()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "\n",
    "missing = housing.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3uqsma30lsl"
   },
   "source": [
    "Remove columns that contain more than 30% of missing data. After removing those columns, remove the rows that contain at least one observation that is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYdSl4kh0lsm"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\ndrop_cols = missing[missing > 0.3].index\\n\\nhousing = housing.drop(drop_cols, 1)\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\ndrop_cols = missing[missing > 0.3].index\\n\\nhousing = housing.drop(drop_cols, 1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "drop_cols = missing[missing > 0.3].index\n",
    "\n",
    "housing = housing.drop(drop_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"housing = housing.dropna()\";\n",
       "                var nbb_formatted_code = \"housing = housing.dropna()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing = housing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape LandContour  \\\n",
       "0   1          60       RL         65.0     8450   Pave      Reg         Lvl   \n",
       "1   2          20       RL         80.0     9600   Pave      Reg         Lvl   \n",
       "2   3          60       RL         68.0    11250   Pave      IR1         Lvl   \n",
       "\n",
       "  Utilities LotConfig  ... EnclosedPorch 3SsnPorch ScreenPorch PoolArea  \\\n",
       "0    AllPub    Inside  ...             0         0           0        0   \n",
       "1    AllPub       FR2  ...             0         0           0        0   \n",
       "2    AllPub    Inside  ...             0         0           0        0   \n",
       "\n",
       "  MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0       0      2    2008        WD         Normal     208500  \n",
       "1       0      5    2007        WD         Normal     181500  \n",
       "2       0      9    2008        WD         Normal     223500  \n",
       "\n",
       "[3 rows x 76 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"housing.head(3)\";\n",
       "                var nbb_formatted_code = \"housing.head(3)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dnm4HNYr0lso"
   },
   "source": [
    "There are some categorical variables that contain numeric data and some that do not. Print the type of each column to first see whether there is an issue with misclassification of column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20     411\n",
       "60     227\n",
       "50     114\n",
       "120     66\n",
       "160     53\n",
       "70      52\n",
       "30      47\n",
       "80      37\n",
       "90      24\n",
       "190     19\n",
       "85      13\n",
       "75      13\n",
       "45       9\n",
       "180      6\n",
       "40       3\n",
       "Name: MSSubClass, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"housing[\\\"MSSubClass\\\"].value_counts()\";\n",
       "                var nbb_formatted_code = \"housing[\\\"MSSubClass\\\"].value_counts()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing[\"MSSubClass\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKKKIkaC0lsp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "MSSubClass         int64\n",
       "MSZoning          object\n",
       "LotFrontage      float64\n",
       "LotArea            int64\n",
       "Street            object\n",
       "LotShape          object\n",
       "LandContour       object\n",
       "Utilities         object\n",
       "LotConfig         object\n",
       "LandSlope         object\n",
       "Neighborhood      object\n",
       "Condition1        object\n",
       "Condition2        object\n",
       "BldgType          object\n",
       "HouseStyle        object\n",
       "OverallQual        int64\n",
       "OverallCond        int64\n",
       "YearBuilt          int64\n",
       "YearRemodAdd       int64\n",
       "RoofStyle         object\n",
       "RoofMatl          object\n",
       "Exterior1st       object\n",
       "Exterior2nd       object\n",
       "MasVnrType        object\n",
       "MasVnrArea       float64\n",
       "ExterQual         object\n",
       "ExterCond         object\n",
       "Foundation        object\n",
       "BsmtQual          object\n",
       "BsmtCond          object\n",
       "BsmtExposure      object\n",
       "BsmtFinType1      object\n",
       "BsmtFinSF1         int64\n",
       "BsmtFinType2      object\n",
       "BsmtFinSF2         int64\n",
       "BsmtUnfSF          int64\n",
       "TotalBsmtSF        int64\n",
       "Heating           object\n",
       "HeatingQC         object\n",
       "CentralAir        object\n",
       "Electrical        object\n",
       "1stFlrSF           int64\n",
       "2ndFlrSF           int64\n",
       "LowQualFinSF       int64\n",
       "GrLivArea          int64\n",
       "BsmtFullBath       int64\n",
       "BsmtHalfBath       int64\n",
       "FullBath           int64\n",
       "HalfBath           int64\n",
       "BedroomAbvGr       int64\n",
       "KitchenAbvGr       int64\n",
       "KitchenQual       object\n",
       "TotRmsAbvGrd       int64\n",
       "Functional        object\n",
       "Fireplaces         int64\n",
       "GarageType        object\n",
       "GarageYrBlt      float64\n",
       "GarageFinish      object\n",
       "GarageCars         int64\n",
       "GarageArea         int64\n",
       "GarageQual        object\n",
       "GarageCond        object\n",
       "PavedDrive        object\n",
       "WoodDeckSF         int64\n",
       "OpenPorchSF        int64\n",
       "EnclosedPorch      int64\n",
       "3SsnPorch          int64\n",
       "ScreenPorch        int64\n",
       "PoolArea           int64\n",
       "MiscVal            int64\n",
       "MoSold             int64\n",
       "YrSold             int64\n",
       "SaleType          object\n",
       "SaleCondition     object\n",
       "SalePrice          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nhousing.dtypes\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nhousing.dtypes\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "housing.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZfA-TZu0lsq"
   },
   "source": [
    "We see that month sold and year sold are not variables that describe a feature of the house. While they do have relevance if we create a model containing a time series element, we will not include them here. Drop these columns. Also, remove the id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lVZUPSWm0lsr"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nhousing = housing.drop(['Id', 'MoSold', 'YrSold'],1)\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nhousing = housing.drop([\\\"Id\\\", \\\"MoSold\\\", \\\"YrSold\\\"], 1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "housing = housing.drop(['Id', 'MoSold', 'YrSold'],1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4OxMXQ8g0lst"
   },
   "source": [
    "Using the information about the column types, identify all the variables that will be converted into dummy variables. Include at least one numeric variable that you think should be converted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"cat_cols = list(housing.select_dtypes(\\\"object\\\").columns)\\n\\ncat_cols.append(\\\"MSZoning\\\")\";\n",
       "                var nbb_formatted_code = \"cat_cols = list(housing.select_dtypes(\\\"object\\\").columns)\\n\\ncat_cols.append(\\\"MSZoning\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_cols = list(housing.select_dtypes(\"object\").columns)\n",
    "\n",
    "cat_cols.append(\"MSZoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6wVL2LA0lsv"
   },
   "source": [
    "Convert the columns you selected above into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bORGylTC0lsv"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\n# housing_cat = housing[cat_cols]\\n\\nh_dummy = pd.get_dummies(housing, columns=cat_cols, drop_first=True)\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\n# housing_cat = housing[cat_cols]\\n\\nh_dummy = pd.get_dummies(housing, columns=cat_cols, drop_first=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "# housing_cat = housing[cat_cols]\n",
    "\n",
    "h_dummy = pd.get_dummies(housing, columns=cat_cols, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQkbLG0e0lsx"
   },
   "source": [
    "Split the data into train and test with 20% of data in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import train_test_split\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import train_test_split\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SivhGDOu0lsx"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nX = h_dummy.drop(\\\"SalePrice\\\", 1)\\ny = np.array(housing[\\\"SalePrice\\\"])\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=34\\n)\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nX = h_dummy.drop(\\\"SalePrice\\\", 1)\\ny = np.array(housing[\\\"SalePrice\\\"])\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    X, y, test_size=0.2, random_state=34\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "X = h_dummy.drop(\"SalePrice\", 1)\n",
    "y = np.array(housing[\"SalePrice\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=34\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"from sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\n\\n# X_train = pd.DataFrame(scaler.fit_transform(X_train))\\n# X_test = pd.DataFrame(scaler.transform(X_test))\\n\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\";\n",
       "                var nbb_formatted_code = \"from sklearn.preprocessing import MinMaxScaler\\n\\nscaler = MinMaxScaler()\\n\\n# X_train = pd.DataFrame(scaler.fit_transform(X_train))\\n# X_test = pd.DataFrame(scaler.transform(X_test))\\n\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "# X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujvHdv2z0lsz"
   },
   "source": [
    "Create a model with 5 layers. The first layer should be a dense layer that takes in the input, the last layer should be of size 1. You determine the remaining layer sizes.\n",
    "\n",
    "Use a linear activation for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EvTExRC0lsz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"# Answer below\\nmodel = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_formatted_code = \"# Answer below\\nmodel = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blQTwPWz0ls1"
   },
   "source": [
    "Compile the model with the RMSprop optimizer and mean square error loss. Use the MSE as a metric. Set batch size to 100 and epochs to 200. Fit the model and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjcqV1Zm0ls1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics = ['mse'])\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nmodel.compile(loss=\\\"mean_squared_error\\\", optimizer=\\\"adam\\\", metrics=[\\\"mse\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "model.compile(loss = 'mean_squared_error', optimizer='adam', metrics = ['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RangeDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RepeatDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ZipDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DatasetCardinality in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 875 samples, validate on 219 samples\n",
      "Epoch 1/200\n",
      "Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027C7D1A1948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027C7D1A1948> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Executing op __inference_initialize_variables_783 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_1028 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "100/875 [==>...........................] - ETA: 9s - loss: 43864850432.0000 - mse: 43864850432.0000Executing op __inference_distributed_function_1151 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "875/875 [==============================] - 2s 2ms/sample - loss: 41958593565.2571 - mse: 41958588416.0000 - val_loss: 41623148815.1964 - val_mse: 41623146496.0000\n",
      "Epoch 2/200\n",
      "100/875 [==>...........................] - ETA: 0s - loss: 46298746880.0000 - mse: 46298746880.0000Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 41957775535.5429 - mse: 41957769216.0000 - val_loss: 41621834429.3699 - val_mse: 41621835776.0000\n",
      "Epoch 3/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 41955696757.0286 - mse: 41955696640.0000 - val_loss: 41618427586.0457 - val_mse: 41618427904.0000\n",
      "Epoch 4/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 41950346912.9143 - mse: 41950347264.0000 - val_loss: 41609802831.4886 - val_mse: 41609801728.0000\n",
      "Epoch 5/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 41937227893.0286 - mse: 41937223680.0000 - val_loss: 41590153141.1872 - val_mse: 41590153216.0000\n",
      "Epoch 6/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 41909158239.0857 - mse: 41909157888.0000 - val_loss: 41549418907.4703 - val_mse: 41549422592.0000\n",
      "Epoch 7/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 41853999455.0857 - mse: 41853997056.0000 - val_loss: 41470550820.2374 - val_mse: 41470550016.0000\n",
      "Epoch 8/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 41748065952.9143 - mse: 41748066304.0000 - val_loss: 41327205820.2009 - val_mse: 41327206400.0000\n",
      "Epoch 9/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 41562466713.6000 - mse: 41562464256.0000 - val_loss: 41079511198.9772 - val_mse: 41079513088.0000\n",
      "Epoch 10/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 41246182019.6571 - mse: 41246183424.0000 - val_loss: 40672732370.4110 - val_mse: 40672735232.0000\n",
      "Epoch 11/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 40744061454.6286 - mse: 40744062976.0000 - val_loss: 40031037954.3379 - val_mse: 40031039488.0000\n",
      "Epoch 12/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 39956218324.1143 - mse: 39956217856.0000 - val_loss: 39061802895.7808 - val_mse: 39061803008.0000\n",
      "Epoch 13/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 38788851419.4286 - mse: 38788849664.0000 - val_loss: 37648478357.6256 - val_mse: 37648478208.0000\n",
      "Epoch 14/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 37128859999.0857 - mse: 37128859648.0000 - val_loss: 35665588513.8995 - val_mse: 35665588224.0000\n",
      "Epoch 15/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 34817334242.7429 - mse: 34817335296.0000 - val_loss: 33005985829.4064 - val_mse: 33005985792.0000\n",
      "Epoch 16/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 31793114375.3143 - mse: 31793115136.0000 - val_loss: 29581046274.3379 - val_mse: 29581047808.0000\n",
      "Epoch 17/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 27990183818.9714 - mse: 27990181888.0000 - val_loss: 25414939007.4155 - val_mse: 25414940672.0000\n",
      "Epoch 18/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 23477174915.6571 - mse: 23477176320.0000 - val_loss: 20680578482.8493 - val_mse: 20680579072.0000\n",
      "Epoch 19/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 18567467242.0571 - mse: 18567464960.0000 - val_loss: 15776176151.3790 - val_mse: 15776177152.0000\n",
      "Epoch 20/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 13700639509.9429 - mse: 13700638720.0000 - val_loss: 11370840498.8493 - val_mse: 11370841088.0000\n",
      "Epoch 21/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 9668421690.5143 - mse: 9668421632.0000 - val_loss: 8179109240.4018 - val_mse: 8179108864.0000\n",
      "Epoch 22/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 7101911917.7143 - mse: 7101911552.0000 - val_loss: 6609202919.4521 - val_mse: 6609203200.0000\n",
      "Epoch 23/200\n",
      "875/875 [==============================] - 0s 37us/sample - loss: 6003362903.7714 - mse: 6003362816.0000 - val_loss: 6216638964.3105 - val_mse: 6216638464.0000\n",
      "Epoch 24/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 5756115470.6286 - mse: 5756115456.0000 - val_loss: 6109832360.3288 - val_mse: 6109832192.0000\n",
      "Epoch 25/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 5599331781.4857 - mse: 5599331840.0000 - val_loss: 5914134661.2603 - val_mse: 5914134528.0000\n",
      "Epoch 26/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 5424054915.6571 - mse: 5424054784.0000 - val_loss: 5723928585.3516 - val_mse: 5723928576.0000\n",
      "Epoch 27/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 5263156399.5429 - mse: 5263156224.0000 - val_loss: 5567534009.8630 - val_mse: 5567534080.0000\n",
      "Epoch 28/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 5124300851.2000 - mse: 5124300288.0000 - val_loss: 5426375523.3607 - val_mse: 5426375168.0000\n",
      "Epoch 29/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 4999947220.1143 - mse: 4999947264.0000 - val_loss: 5295817669.5525 - val_mse: 5295817728.0000\n",
      "Epoch 30/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 4881320842.9714 - mse: 4881320960.0000 - val_loss: 5169053014.5023 - val_mse: 5169053184.0000\n",
      "Epoch 31/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 4762814756.5714 - mse: 4762814976.0000 - val_loss: 5048324901.4064 - val_mse: 5048324608.0000\n",
      "Epoch 32/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 4654369440.9143 - mse: 4654369280.0000 - val_loss: 4930533293.0046 - val_mse: 4930533376.0000\n",
      "Epoch 33/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 4547340434.2857 - mse: 4547340288.0000 - val_loss: 4818090186.2283 - val_mse: 4818089984.0000\n",
      "Epoch 34/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 4444253008.4571 - mse: 4444253184.0000 - val_loss: 4711267569.9726 - val_mse: 4711267328.0000\n",
      "Epoch 35/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 4347265623.7714 - mse: 4347265536.0000 - val_loss: 4608534986.2283 - val_mse: 4608535040.0000\n",
      "Epoch 36/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 4261030165.9429 - mse: 4261030400.0000 - val_loss: 4506879395.6530 - val_mse: 4506879488.0000\n",
      "Epoch 37/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 4167001468.3429 - mse: 4167001088.0000 - val_loss: 4406980904.9132 - val_mse: 4406980608.0000\n",
      "Epoch 38/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 4075078692.5714 - mse: 4075078912.0000 - val_loss: 4316623753.9361 - val_mse: 4316623872.0000\n",
      "Epoch 39/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 3991430421.9429 - mse: 3991429888.0000 - val_loss: 4231716984.4018 - val_mse: 4231716864.0000\n",
      "Epoch 40/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 3921217104.4571 - mse: 3921217280.0000 - val_loss: 4137745376.4384 - val_mse: 4137745408.0000\n",
      "Epoch 41/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 3834362027.8857 - mse: 3834362368.0000 - val_loss: 4055911914.9589 - val_mse: 4055911680.0000\n",
      "Epoch 42/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 3759172242.2857 - mse: 3759171840.0000 - val_loss: 3974644011.2511 - val_mse: 3974643968.0000\n",
      "Epoch 43/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 3685549948.3429 - mse: 3685549824.0000 - val_loss: 3897292349.9543 - val_mse: 3897292544.0000\n",
      "Epoch 44/200\n",
      "875/875 [==============================] - 0s 37us/sample - loss: 3616347560.2286 - mse: 3616347904.0000 - val_loss: 3820577630.6849 - val_mse: 3820577536.0000\n",
      "Epoch 45/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 3553002803.2000 - mse: 3553002752.0000 - val_loss: 3743819098.0091 - val_mse: 3743818752.0000\n",
      "Epoch 46/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 3481028132.5714 - mse: 3481028096.0000 - val_loss: 3675745435.4703 - val_mse: 3675745280.0000\n",
      "Epoch 47/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 3418844869.4857 - mse: 3418844928.0000 - val_loss: 3607146888.7671 - val_mse: 3607147008.0000\n",
      "Epoch 48/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 3358001342.1714 - mse: 3358001408.0000 - val_loss: 3539908724.8950 - val_mse: 3539908864.0000\n",
      "Epoch 49/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 3297414963.2000 - mse: 3297414656.0000 - val_loss: 3476276595.7260 - val_mse: 3476276736.0000\n",
      "Epoch 50/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 3240579452.3429 - mse: 3240579072.0000 - val_loss: 3408916133.9909 - val_mse: 3408915968.0000\n",
      "Epoch 51/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 3184650752.0000 - mse: 3184650752.0000 - val_loss: 3352967221.7717 - val_mse: 3352967168.0000\n",
      "Epoch 52/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 3130154905.6000 - mse: 3130155264.0000 - val_loss: 3288758013.6621 - val_mse: 3288758016.0000\n",
      "Epoch 53/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 3077179040.9143 - mse: 3077179392.0000 - val_loss: 3235325154.7763 - val_mse: 3235324928.0000\n",
      "Epoch 54/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 3029426102.8571 - mse: 3029425920.0000 - val_loss: 3173928638.5388 - val_mse: 3173928704.0000\n",
      "Epoch 55/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2975281276.3429 - mse: 2975281664.0000 - val_loss: 3121718852.9680 - val_mse: 3121719040.0000\n",
      "Epoch 56/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2927253109.0286 - mse: 2927252992.0000 - val_loss: 3072473841.9726 - val_mse: 3072473600.0000\n",
      "Epoch 57/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2881374837.0286 - mse: 2881374720.0000 - val_loss: 3026922307.7991 - val_mse: 3026922240.0000\n",
      "Epoch 58/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2836749143.7714 - mse: 2836749312.0000 - val_loss: 2976666033.6804 - val_mse: 2976666112.0000\n",
      "Epoch 59/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2795751541.0286 - mse: 2795751424.0000 - val_loss: 2925147392.0000 - val_mse: 2925147648.0000\n",
      "Epoch 60/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 2756854875.4286 - mse: 2756854528.0000 - val_loss: 2874919754.8128 - val_mse: 2874919680.0000\n",
      "Epoch 61/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 2708536517.4857 - mse: 2708536576.0000 - val_loss: 2839436752.6575 - val_mse: 2839436800.0000\n",
      "Epoch 62/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2671450104.6857 - mse: 2671450112.0000 - val_loss: 2800775459.0685 - val_mse: 2800775424.0000\n",
      "Epoch 63/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 2634027925.9429 - mse: 2634027776.0000 - val_loss: 2754010494.8311 - val_mse: 2754010368.0000\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 0s 42us/sample - loss: 2596383967.0857 - mse: 2596383744.0000 - val_loss: 2711117603.0685 - val_mse: 2711117824.0000\n",
      "Epoch 65/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2558685465.6000 - mse: 2558685440.0000 - val_loss: 2672890887.0137 - val_mse: 2672890880.0000\n",
      "Epoch 66/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 2523225797.4857 - mse: 2523225600.0000 - val_loss: 2634120409.4247 - val_mse: 2634120448.0000\n",
      "Epoch 67/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2490333611.8857 - mse: 2490333696.0000 - val_loss: 2590790308.8219 - val_mse: 2590790400.0000\n",
      "Epoch 68/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 2454104846.6286 - mse: 2454104832.0000 - val_loss: 2558798800.6575 - val_mse: 2558798848.0000\n",
      "Epoch 69/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2422624877.7143 - mse: 2422625024.0000 - val_loss: 2517410019.9452 - val_mse: 2517410048.0000\n",
      "Epoch 70/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2390311380.1143 - mse: 2390311424.0000 - val_loss: 2492020747.6895 - val_mse: 2492020736.0000\n",
      "Epoch 71/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 2355621493.0286 - mse: 2355621120.0000 - val_loss: 2453227683.6530 - val_mse: 2453227776.0000\n",
      "Epoch 72/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 2327891602.2857 - mse: 2327891712.0000 - val_loss: 2410541365.1872 - val_mse: 2410541312.0000\n",
      "Epoch 73/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 2295746965.9429 - mse: 2295747072.0000 - val_loss: 2377677504.2922 - val_mse: 2377677568.0000\n",
      "Epoch 74/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2266357013.9429 - mse: 2266356992.0000 - val_loss: 2347307871.2694 - val_mse: 2347307776.0000\n",
      "Epoch 75/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 2234111005.2571 - mse: 2234110976.0000 - val_loss: 2325901507.2146 - val_mse: 2325901568.0000\n",
      "Epoch 76/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2207532392.2286 - mse: 2207532288.0000 - val_loss: 2294035987.2877 - val_mse: 2294035968.0000\n",
      "Epoch 77/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2180735839.0857 - mse: 2180735744.0000 - val_loss: 2253437011.5799 - val_mse: 2253436928.0000\n",
      "Epoch 78/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 2150558006.8571 - mse: 2150557952.0000 - val_loss: 2230161590.9406 - val_mse: 2230161664.0000\n",
      "Epoch 79/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 2124549888.0000 - mse: 2124549888.0000 - val_loss: 2203546228.8950 - val_mse: 2203546368.0000\n",
      "Epoch 80/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2099849409.8286 - mse: 2099849344.0000 - val_loss: 2165403804.6393 - val_mse: 2165403904.0000\n",
      "Epoch 81/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 2070281786.5143 - mse: 2070281600.0000 - val_loss: 2140774214.1370 - val_mse: 2140774144.0000\n",
      "Epoch 82/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2045983641.6000 - mse: 2045983616.0000 - val_loss: 2116281473.1689 - val_mse: 2116281344.0000\n",
      "Epoch 83/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2020082146.7429 - mse: 2020082176.0000 - val_loss: 2087007952.0731 - val_mse: 2087007872.0000\n",
      "Epoch 84/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1999148500.1143 - mse: 1999148544.0000 - val_loss: 2048418101.1872 - val_mse: 2048418048.0000\n",
      "Epoch 85/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1971925045.0286 - mse: 1971925248.0000 - val_loss: 2027463521.6073 - val_mse: 2027463552.0000\n",
      "Epoch 86/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1950018271.0857 - mse: 1950018176.0000 - val_loss: 2016937275.0320 - val_mse: 2016937472.0000\n",
      "Epoch 87/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1924680115.2000 - mse: 1924680064.0000 - val_loss: 1981037727.5616 - val_mse: 1981037568.0000\n",
      "Epoch 88/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1903617484.8000 - mse: 1903617664.0000 - val_loss: 1950235176.9132 - val_mse: 1950235264.0000\n",
      "Epoch 89/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1879997330.2857 - mse: 1879997312.0000 - val_loss: 1930375390.1005 - val_mse: 1930375424.0000\n",
      "Epoch 90/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1858251285.9429 - mse: 1858251136.0000 - val_loss: 1902903918.4658 - val_mse: 1902904064.0000\n",
      "Epoch 91/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1835479040.0000 - mse: 1835478912.0000 - val_loss: 1879650537.2055 - val_mse: 1879650560.0000\n",
      "Epoch 92/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1815511498.9714 - mse: 1815511552.0000 - val_loss: 1862731083.9817 - val_mse: 1862731136.0000\n",
      "Epoch 93/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1796669739.8857 - mse: 1796669824.0000 - val_loss: 1830582807.9635 - val_mse: 1830582656.0000\n",
      "Epoch 94/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1774157851.4286 - mse: 1774157952.0000 - val_loss: 1807954900.7489 - val_mse: 1807954944.0000\n",
      "Epoch 95/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1753525986.7429 - mse: 1753526016.0000 - val_loss: 1796927831.6712 - val_mse: 1796927872.0000\n",
      "Epoch 96/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1733873411.6571 - mse: 1733873280.0000 - val_loss: 1772626239.7078 - val_mse: 1772626304.0000\n",
      "Epoch 97/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1714950491.4286 - mse: 1714950528.0000 - val_loss: 1744718767.3425 - val_mse: 1744718720.0000\n",
      "Epoch 98/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1696365569.8286 - mse: 1696365568.0000 - val_loss: 1717810646.5023 - val_mse: 1717810688.0000\n",
      "Epoch 99/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1678536305.3714 - mse: 1678536448.0000 - val_loss: 1700996211.1416 - val_mse: 1700996352.0000\n",
      "Epoch 100/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1662434293.0286 - mse: 1662434432.0000 - val_loss: 1690240033.3151 - val_mse: 1690239872.0000\n",
      "Epoch 101/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1643452284.3429 - mse: 1643452160.0000 - val_loss: 1669767984.5114 - val_mse: 1669767936.0000\n",
      "Epoch 102/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1626180260.5714 - mse: 1626180224.0000 - val_loss: 1651834383.1963 - val_mse: 1651834368.0000\n",
      "Epoch 103/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1610450750.1714 - mse: 1610450688.0000 - val_loss: 1623489996.5662 - val_mse: 1623489920.0000\n",
      "Epoch 104/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 1594271663.5429 - mse: 1594271744.0000 - val_loss: 1610671737.5708 - val_mse: 1610671872.0000\n",
      "Epoch 105/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 1579610869.0286 - mse: 1579610880.0000 - val_loss: 1594045890.6301 - val_mse: 1594045952.0000\n",
      "Epoch 106/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1565493504.0000 - mse: 1565493376.0000 - val_loss: 1577885209.7169 - val_mse: 1577885312.0000\n",
      "Epoch 107/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1550704544.9143 - mse: 1550704640.0000 - val_loss: 1553056864.4384 - val_mse: 1553056896.0000\n",
      "Epoch 108/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1535184212.1143 - mse: 1535184128.0000 - val_loss: 1548840232.3288 - val_mse: 1548840320.0000\n",
      "Epoch 109/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1521434282.0571 - mse: 1521434368.0000 - val_loss: 1528621521.8265 - val_mse: 1528621568.0000\n",
      "Epoch 110/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1509156492.8000 - mse: 1509156736.0000 - val_loss: 1511960887.5251 - val_mse: 1511960832.0000\n",
      "Epoch 111/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1494750851.6571 - mse: 1494750720.0000 - val_loss: 1492830297.4247 - val_mse: 1492830336.0000\n",
      "Epoch 112/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1484133002.9714 - mse: 1484132992.0000 - val_loss: 1474560637.0776 - val_mse: 1474560640.0000\n",
      "Epoch 113/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1470580423.3143 - mse: 1470580352.0000 - val_loss: 1477997717.6256 - val_mse: 1477997824.0000\n",
      "Epoch 114/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1457856852.1143 - mse: 1457856768.0000 - val_loss: 1458121248.1461 - val_mse: 1458121216.0000\n",
      "Epoch 115/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1444255992.6857 - mse: 1444255872.0000 - val_loss: 1437844208.8037 - val_mse: 1437844096.0000\n",
      "Epoch 116/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1434864190.1714 - mse: 1434864256.0000 - val_loss: 1427705775.9269 - val_mse: 1427705600.0000\n",
      "Epoch 117/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1424941774.6286 - mse: 1424941952.0000 - val_loss: 1403737675.3973 - val_mse: 1403737600.0000\n",
      "Epoch 118/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1410595971.6571 - mse: 1410595968.0000 - val_loss: 1397647782.5753 - val_mse: 1397647616.0000\n",
      "Epoch 119/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1400447372.8000 - mse: 1400447360.0000 - val_loss: 1389066978.7763 - val_mse: 1389066880.0000\n",
      "Epoch 120/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1391783411.2000 - mse: 1391783424.0000 - val_loss: 1378351740.4932 - val_mse: 1378351744.0000\n",
      "Epoch 121/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1380968817.3714 - mse: 1380968704.0000 - val_loss: 1371100061.8082 - val_mse: 1371100032.0000\n",
      "Epoch 122/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1379070248.2286 - mse: 1379070208.0000 - val_loss: 1340101369.5708 - val_mse: 1340101376.0000\n",
      "Epoch 123/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1362631562.9714 - mse: 1362631552.0000 - val_loss: 1352993270.0639 - val_mse: 1352993280.0000\n",
      "Epoch 124/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1355052055.7714 - mse: 1355052032.0000 - val_loss: 1328445047.2329 - val_mse: 1328444928.0000\n",
      "Epoch 125/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1343589065.1429 - mse: 1343589120.0000 - val_loss: 1315416038.5753 - val_mse: 1315416192.0000\n",
      "Epoch 126/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1334132754.2857 - mse: 1334132736.0000 - val_loss: 1316836994.0457 - val_mse: 1316836992.0000\n",
      "Epoch 127/200\n",
      "875/875 [==============================] - 0s 37us/sample - loss: 1328903325.2571 - mse: 1328903168.0000 - val_loss: 1313672603.7626 - val_mse: 1313672576.0000\n",
      "Epoch 128/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1316804196.5714 - mse: 1316804224.0000 - val_loss: 1283554231.5251 - val_mse: 1283554304.0000\n",
      "Epoch 129/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1310567568.4571 - mse: 1310567552.0000 - val_loss: 1279593385.4977 - val_mse: 1279593344.0000\n",
      "Epoch 130/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1302496636.3429 - mse: 1302496768.0000 - val_loss: 1270811273.9361 - val_mse: 1270811264.0000\n",
      "Epoch 131/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1295913329.3714 - mse: 1295913344.0000 - val_loss: 1258147842.6301 - val_mse: 1258147840.0000\n",
      "Epoch 132/200\n",
      "875/875 [==============================] - 0s 49us/sample - loss: 1286832053.0286 - mse: 1286832128.0000 - val_loss: 1265486079.7078 - val_mse: 1265486080.0000\n",
      "Epoch 133/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 1280892097.8286 - mse: 1280892032.0000 - val_loss: 1238031928.9863 - val_mse: 1238031872.0000\n",
      "Epoch 134/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 1273291552.9143 - mse: 1273291520.0000 - val_loss: 1236489322.6667 - val_mse: 1236489344.0000\n",
      "Epoch 135/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 1264639888.4571 - mse: 1264639872.0000 - val_loss: 1225824594.7032 - val_mse: 1225824512.0000\n",
      "Epoch 136/200\n",
      "875/875 [==============================] - 0s 49us/sample - loss: 1259030506.0571 - mse: 1259030400.0000 - val_loss: 1233621865.4977 - val_mse: 1233621888.0000\n",
      "Epoch 137/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 1249515430.4000 - mse: 1249515392.0000 - val_loss: 1210683615.2694 - val_mse: 1210683648.0000\n",
      "Epoch 138/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 1243405194.9714 - mse: 1243405184.0000 - val_loss: 1203625677.4429 - val_mse: 1203625600.0000\n",
      "Epoch 139/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1237351808.0000 - mse: 1237351936.0000 - val_loss: 1192222089.9361 - val_mse: 1192222080.0000\n",
      "Epoch 140/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1232812891.4286 - mse: 1232812800.0000 - val_loss: 1198963369.7900 - val_mse: 1198963328.0000\n",
      "Epoch 141/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1226563018.9714 - mse: 1226563072.0000 - val_loss: 1178815969.3151 - val_mse: 1178815872.0000\n",
      "Epoch 142/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1219287332.5714 - mse: 1219287296.0000 - val_loss: 1183407507.5799 - val_mse: 1183407488.0000\n",
      "Epoch 143/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1212656804.5714 - mse: 1212656896.0000 - val_loss: 1165514073.4247 - val_mse: 1165514112.0000\n",
      "Epoch 144/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1210480629.0286 - mse: 1210480640.0000 - val_loss: 1161863185.2420 - val_mse: 1161863168.0000\n",
      "Epoch 145/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1200147755.8857 - mse: 1200147712.0000 - val_loss: 1159684619.3973 - val_mse: 1159684608.0000\n",
      "Epoch 146/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 1195809120.9143 - mse: 1195809152.0000 - val_loss: 1158175651.6530 - val_mse: 1158175744.0000\n",
      "Epoch 147/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 1188889091.6571 - mse: 1188889088.0000 - val_loss: 1151151342.7580 - val_mse: 1151151232.0000\n",
      "Epoch 148/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1183580226.7429 - mse: 1183580288.0000 - val_loss: 1139953635.6530 - val_mse: 1139953664.0000\n",
      "Epoch 149/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1182163554.7429 - mse: 1182163584.0000 - val_loss: 1128203075.7991 - val_mse: 1128203136.0000\n",
      "Epoch 150/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 1172148181.9429 - mse: 1172148096.0000 - val_loss: 1140433668.3836 - val_mse: 1140433664.0000\n",
      "Epoch 151/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1170195430.4000 - mse: 1170195328.0000 - val_loss: 1121592655.7808 - val_mse: 1121592576.0000\n",
      "Epoch 152/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1164365257.1429 - mse: 1164365312.0000 - val_loss: 1133152556.1279 - val_mse: 1133152512.0000\n",
      "Epoch 153/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1157498222.6286 - mse: 1157498240.0000 - val_loss: 1117032930.7763 - val_mse: 1117032960.0000\n",
      "Epoch 154/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1152530115.6571 - mse: 1152530048.0000 - val_loss: 1102878916.0913 - val_mse: 1102878976.0000\n",
      "Epoch 155/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1147546836.1143 - mse: 1147546880.0000 - val_loss: 1104732323.0685 - val_mse: 1104732288.0000\n",
      "Epoch 156/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1147413908.1143 - mse: 1147413888.0000 - val_loss: 1108070994.1187 - val_mse: 1108071040.0000\n",
      "Epoch 157/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 1136227357.2571 - mse: 1136227456.0000 - val_loss: 1095143191.3790 - val_mse: 1095143168.0000\n",
      "Epoch 158/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 0s 38us/sample - loss: 1134482121.1429 - mse: 1134482048.0000 - val_loss: 1085078520.9863 - val_mse: 1085078528.0000\n",
      "Epoch 159/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1128329095.3143 - mse: 1128329088.0000 - val_loss: 1088903453.2237 - val_mse: 1088903424.0000\n",
      "Epoch 160/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1124487208.2286 - mse: 1124487168.0000 - val_loss: 1094878817.3151 - val_mse: 1094878848.0000\n",
      "Epoch 161/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1121193030.4000 - mse: 1121193088.0000 - val_loss: 1077281354.5205 - val_mse: 1077281408.0000\n",
      "Epoch 162/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1115263401.1429 - mse: 1115263360.0000 - val_loss: 1079330507.9817 - val_mse: 1079330432.0000\n",
      "Epoch 163/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1112079385.6000 - mse: 1112079360.0000 - val_loss: 1070557378.3379 - val_mse: 1070557312.0000\n",
      "Epoch 164/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1103942209.8286 - mse: 1103942144.0000 - val_loss: 1072590654.5388 - val_mse: 1072590592.0000\n",
      "Epoch 165/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1100826372.5714 - mse: 1100826368.0000 - val_loss: 1064725145.1324 - val_mse: 1064725184.0000\n",
      "Epoch 166/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1096409395.2000 - mse: 1096409472.0000 - val_loss: 1060529857.7534 - val_mse: 1060529856.0000\n",
      "Epoch 167/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1091358606.6286 - mse: 1091358592.0000 - val_loss: 1061303175.8904 - val_mse: 1061303232.0000\n",
      "Epoch 168/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1087483145.1429 - mse: 1087483136.0000 - val_loss: 1049755841.1689 - val_mse: 1049755776.0000\n",
      "Epoch 169/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1084038458.5143 - mse: 1084038400.0000 - val_loss: 1044205731.3607 - val_mse: 1044205760.0000\n",
      "Epoch 170/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1078082651.4286 - mse: 1078082688.0000 - val_loss: 1045816283.4703 - val_mse: 1045816320.0000\n",
      "Epoch 171/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1082332191.0857 - mse: 1082332160.0000 - val_loss: 1056150461.6621 - val_mse: 1056150464.0000\n",
      "Epoch 172/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1069997959.3143 - mse: 1069997952.0000 - val_loss: 1027610996.0183 - val_mse: 1027611008.0000\n",
      "Epoch 173/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1067742648.6857 - mse: 1067742720.0000 - val_loss: 1029173274.3014 - val_mse: 1029173312.0000\n",
      "Epoch 174/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1062244955.4286 - mse: 1062244928.0000 - val_loss: 1032336826.1553 - val_mse: 1032336832.0000\n",
      "Epoch 175/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1059603569.3714 - mse: 1059603520.0000 - val_loss: 1032968340.1644 - val_mse: 1032968320.0000\n",
      "Epoch 176/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1058364807.3143 - mse: 1058364800.0000 - val_loss: 1013396113.8265 - val_mse: 1013396096.0000\n",
      "Epoch 177/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1049945142.8571 - mse: 1049945152.0000 - val_loss: 1019204836.8219 - val_mse: 1019204864.0000\n",
      "Epoch 178/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1046890408.2286 - mse: 1046890368.0000 - val_loss: 1024668535.5251 - val_mse: 1024668544.0000\n",
      "Epoch 179/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 1045078979.6571 - mse: 1045078976.0000 - val_loss: 1009978649.4247 - val_mse: 1009978688.0000\n",
      "Epoch 180/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 1041756609.8286 - mse: 1041756608.0000 - val_loss: 1022064748.7123 - val_mse: 1022064768.0000\n",
      "Epoch 181/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1034502378.0571 - mse: 1034502464.0000 - val_loss: 1006788553.0594 - val_mse: 1006788544.0000\n",
      "Epoch 182/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1042755382.8571 - mse: 1042755328.0000 - val_loss: 990142059.5434 - val_mse: 990142144.0000\n",
      "Epoch 183/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1034278078.1714 - mse: 1034278080.0000 - val_loss: 1029884768.1461 - val_mse: 1029884800.0000\n",
      "Epoch 184/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1028561305.6000 - mse: 1028561408.0000 - val_loss: 998369078.9406 - val_mse: 998369152.0000\n",
      "Epoch 185/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 1020373258.9714 - mse: 1020373248.0000 - val_loss: 989314407.4521 - val_mse: 989314304.0000\n",
      "Epoch 186/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 1017594536.2286 - mse: 1017594560.0000 - val_loss: 991594706.4110 - val_mse: 991594752.0000\n",
      "Epoch 187/200\n",
      "875/875 [==============================] - 0s 35us/sample - loss: 1016338342.4000 - mse: 1016338304.0000 - val_loss: 992347129.2785 - val_mse: 992347072.0000\n",
      "Epoch 188/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1011913764.5714 - mse: 1011913728.0000 - val_loss: 992451590.7215 - val_mse: 992451584.0000\n",
      "Epoch 189/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1007115269.4857 - mse: 1007115328.0000 - val_loss: 979335209.7900 - val_mse: 979335168.0000\n",
      "Epoch 190/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 1004708033.8286 - mse: 1004708096.0000 - val_loss: 974833740.8584 - val_mse: 974833792.0000\n",
      "Epoch 191/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1001126164.1143 - mse: 1001126144.0000 - val_loss: 977998981.5525 - val_mse: 977998976.0000\n",
      "Epoch 192/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 996931237.4857 - mse: 996931328.0000 - val_loss: 985994433.4612 - val_mse: 985994368.0000\n",
      "Epoch 193/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 994284854.8571 - mse: 994284800.0000 - val_loss: 977569593.5708 - val_mse: 977569600.0000\n",
      "Epoch 194/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 992206397.2571 - mse: 992206336.0000 - val_loss: 964413390.6119 - val_mse: 964413440.0000\n",
      "Epoch 195/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 987299582.1714 - mse: 987299520.0000 - val_loss: 973379253.4795 - val_mse: 973379264.0000\n",
      "Epoch 196/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 987764977.3714 - mse: 987764928.0000 - val_loss: 978051728.9498 - val_mse: 978051712.0000\n",
      "Epoch 197/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 981451827.2000 - mse: 981451712.0000 - val_loss: 956984906.2283 - val_mse: 956984960.0000\n",
      "Epoch 198/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 979523459.6571 - mse: 979523456.0000 - val_loss: 962496320.0000 - val_mse: 962496320.0000\n",
      "Epoch 199/200\n",
      "875/875 [==============================] - 0s 36us/sample - loss: 976000603.4286 - mse: 976000512.0000 - val_loss: 955303335.1598 - val_mse: 955303296.0000\n",
      "Epoch 200/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 972746222.6286 - mse: 972746176.0000 - val_loss: 964746967.0868 - val_mse: 964746944.0000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_formatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"df = pd.DataFrame(history.history)\\n\\ndf[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_formatted_code = \"df = pd.DataFrame(history.history)\\n\\ndf[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "\n",
    "df[[\"loss\", \"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pD1byILV0ls3"
   },
   "source": [
    "Next, do the same but with mean absolute error loss. Use both MSE and MAE as metrics. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"model = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_formatted_code = \"model = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xz0cnwUq0ls4"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nmodel.compile(loss = 'mean_absolute_error', optimizer='adam', metrics = ['mse', 'mae'])\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nmodel.compile(loss=\\\"mean_absolute_error\\\", optimizer=\\\"adam\\\", metrics=[\\\"mse\\\", \\\"mae\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "model.compile(loss = 'mean_absolute_error', optimizer='adam', metrics = ['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 875 samples, validate on 219 samples\n",
      "Epoch 1/200\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027C7B363828> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027C7B363828> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Executing op __inference_initialize_variables_10543 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_10803 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "100/875 [==>...........................] - ETA: 8s - loss: 180618.8594 - mse: 38935363584.0000 - mae: 180618.8594Executing op __inference_distributed_function_10954 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "875/875 [==============================] - 2s 2ms/sample - loss: 187498.1540 - mse: 41957957632.0000 - mae: 187498.1562 - val_loss: 185164.1247 - val_mse: 41621979136.0000 - val_mae: 185164.1250\n",
      "Epoch 2/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 187492.5580 - mse: 41955794944.0000 - mae: 187492.5781 - val_loss: 185154.9294 - val_mse: 41618522112.0000 - val_mae: 185154.9375\n",
      "Epoch 3/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 187478.3379 - mse: 41950445568.0000 - mae: 187478.3438 - val_loss: 185131.6229 - val_mse: 41609764864.0000 - val_mae: 185131.6094\n",
      "Epoch 4/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 187443.0987 - mse: 41937133568.0000 - mae: 187443.0938 - val_loss: 185076.0149 - val_mse: 41588875264.0000 - val_mae: 185076.0156\n",
      "Epoch 5/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 187362.2058 - mse: 41906409472.0000 - mae: 187362.1875 - val_loss: 184953.2979 - val_mse: 41542823936.0000 - val_mae: 184953.2969\n",
      "Epoch 6/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 187190.8603 - mse: 41840529408.0000 - mae: 187190.8594 - val_loss: 184704.5186 - val_mse: 41449574400.0000 - val_mae: 184704.5156\n",
      "Epoch 7/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 186856.0598 - mse: 41714491392.0000 - mae: 186856.0469 - val_loss: 184233.5377 - val_mse: 41273393152.0000 - val_mae: 184233.5312\n",
      "Epoch 8/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 186238.5134 - mse: 41482940416.0000 - mae: 186238.5156 - val_loss: 183388.9297 - val_mse: 40958636032.0000 - val_mae: 183388.9375\n",
      "Epoch 9/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 185157.6156 - mse: 41075970048.0000 - mae: 185157.6094 - val_loss: 181950.8017 - val_mse: 40426110976.0000 - val_mae: 181950.7969\n",
      "Epoch 10/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 183356.5536 - mse: 40403677184.0000 - mae: 183356.5625 - val_loss: 179606.1661 - val_mse: 39566790656.0000 - val_mae: 179606.1562\n",
      "Epoch 11/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 180478.3875 - mse: 39348445184.0000 - mae: 180478.3906 - val_loss: 175933.5077 - val_mse: 38242922496.0000 - val_mae: 175933.5156\n",
      "Epoch 12/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 176042.8634 - mse: 37739425792.0000 - mae: 176042.8594 - val_loss: 170378.2579 - val_mse: 36292194304.0000 - val_mae: 170378.2656\n",
      "Epoch 13/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 169426.6018 - mse: 35449880576.0000 - mae: 169426.5938 - val_loss: 162217.1285 - val_mse: 33538906112.0000 - val_mae: 162217.1250\n",
      "Epoch 14/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 159822.2737 - mse: 32233652224.0000 - mae: 159822.2812 - val_loss: 150531.0090 - val_mse: 29828958208.0000 - val_mae: 150531.0156\n",
      "Epoch 15/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 146225.4868 - mse: 28057231360.0000 - mae: 146225.4844 - val_loss: 134187.3303 - val_mse: 25099988992.0000 - val_mae: 134187.3281\n",
      "Epoch 16/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 127417.5891 - mse: 22816886784.0000 - mae: 127417.6016 - val_loss: 111916.1216 - val_mse: 19502272512.0000 - val_mae: 111916.1172\n",
      "Epoch 17/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 102311.7556 - mse: 16849490944.0000 - mae: 102311.7656 - val_loss: 83497.6168 - val_mse: 13586120704.0000 - val_mae: 83497.6172\n",
      "Epoch 18/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 72511.9456 - mse: 11146847232.0000 - mae: 72511.9453 - val_loss: 57418.7327 - val_mse: 8677339136.0000 - val_mae: 57418.7305\n",
      "Epoch 19/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 54073.8829 - mse: 7108533248.0000 - mae: 54073.8789 - val_loss: 53675.8673 - val_mse: 6586580992.0000 - val_mae: 53675.8711\n",
      "Epoch 20/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 52731.8330 - mse: 5962770944.0000 - mae: 52731.8320 - val_loss: 54182.2858 - val_mse: 6207415296.0000 - val_mae: 54182.2891\n",
      "Epoch 21/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 51388.6703 - mse: 5763641856.0000 - mae: 51388.6680 - val_loss: 51088.7070 - val_mse: 6149140992.0000 - val_mae: 51088.7109\n",
      "Epoch 22/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 49114.4175 - mse: 5832028672.0000 - mae: 49114.4180 - val_loss: 49070.9913 - val_mse: 6243757568.0000 - val_mae: 49070.9922\n",
      "Epoch 23/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 47765.5104 - mse: 5896137216.0000 - mae: 47765.5078 - val_loss: 47915.5991 - val_mse: 6156834304.0000 - val_mae: 47915.5977\n",
      "Epoch 24/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 46503.7006 - mse: 5676780032.0000 - mae: 46503.7031 - val_loss: 46746.2771 - val_mse: 5830102528.0000 - val_mae: 46746.2773\n",
      "Epoch 25/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 45323.1559 - mse: 5386484224.0000 - mae: 45323.1523 - val_loss: 45722.6993 - val_mse: 5604421632.0000 - val_mae: 45722.6992\n",
      "Epoch 26/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 44271.5358 - mse: 5229005312.0000 - mae: 44271.5352 - val_loss: 44634.9836 - val_mse: 5504492032.0000 - val_mae: 44634.9844\n",
      "Epoch 27/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 43249.0811 - mse: 5113527296.0000 - mae: 43249.0781 - val_loss: 43664.1183 - val_mse: 5340161024.0000 - val_mae: 43664.1172\n",
      "Epoch 28/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 42312.3054 - mse: 4967545344.0000 - mae: 42312.3047 - val_loss: 42712.6084 - val_mse: 5228321280.0000 - val_mae: 42712.6055\n",
      "Epoch 29/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 41479.9559 - mse: 4850768384.0000 - mae: 41479.9531 - val_loss: 41842.2660 - val_mse: 5079480320.0000 - val_mae: 41842.2656\n",
      "Epoch 30/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 40678.8575 - mse: 4771127808.0000 - mae: 40678.8633 - val_loss: 41060.9106 - val_mse: 5047095296.0000 - val_mae: 41060.9102\n",
      "Epoch 31/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 39973.9638 - mse: 4694670848.0000 - mae: 39973.9648 - val_loss: 40303.8727 - val_mse: 4880233472.0000 - val_mae: 40303.8711\n",
      "Epoch 32/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 39264.3760 - mse: 4532596736.0000 - mae: 39264.3789 - val_loss: 39602.1142 - val_mse: 4752435200.0000 - val_mae: 39602.1133\n",
      "Epoch 33/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 38622.3012 - mse: 4421974528.0000 - mae: 38622.3008 - val_loss: 38935.7651 - val_mse: 4658226688.0000 - val_mae: 38935.7656\n",
      "Epoch 34/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 38012.1964 - mse: 4337170944.0000 - mae: 38012.1953 - val_loss: 38331.4461 - val_mse: 4576842752.0000 - val_mae: 38331.4492\n",
      "Epoch 35/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 37464.2545 - mse: 4244524544.0000 - mae: 37464.2578 - val_loss: 37722.9663 - val_mse: 4479114752.0000 - val_mae: 37722.9688\n",
      "Epoch 36/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 36949.7825 - mse: 4136489472.0000 - mae: 36949.7812 - val_loss: 37097.6853 - val_mse: 4340675584.0000 - val_mae: 37097.6836\n",
      "Epoch 37/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 36451.3015 - mse: 4056841984.0000 - mae: 36451.3008 - val_loss: 36535.6661 - val_mse: 4282598144.0000 - val_mae: 36535.6680\n",
      "Epoch 38/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 35977.2388 - mse: 3949697280.0000 - mae: 35977.2383 - val_loss: 35963.1122 - val_mse: 4183929344.0000 - val_mae: 35963.1133\n",
      "Epoch 39/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 35514.9074 - mse: 3895776512.0000 - mae: 35514.9062 - val_loss: 35434.4403 - val_mse: 4127184384.0000 - val_mae: 35434.4414\n",
      "Epoch 40/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 35079.3339 - mse: 3830348288.0000 - mae: 35079.3359 - val_loss: 34946.4005 - val_mse: 4045319936.0000 - val_mae: 34946.3984\n",
      "Epoch 41/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 34688.9929 - mse: 3724812800.0000 - mae: 34688.9922 - val_loss: 34459.1430 - val_mse: 3937573376.0000 - val_mae: 34459.1445\n",
      "Epoch 42/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 34313.2209 - mse: 3654924288.0000 - mae: 34313.2188 - val_loss: 34013.0502 - val_mse: 3861576448.0000 - val_mae: 34013.0508\n",
      "Epoch 43/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 33931.3644 - mse: 3589986816.0000 - mae: 33931.3633 - val_loss: 33604.9311 - val_mse: 3828721664.0000 - val_mae: 33604.9297\n",
      "Epoch 44/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 33579.8138 - mse: 3523802624.0000 - mae: 33579.8125 - val_loss: 33158.8291 - val_mse: 3724773376.0000 - val_mae: 33158.8320\n",
      "Epoch 45/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 33237.3996 - mse: 3459973120.0000 - mae: 33237.4023 - val_loss: 32789.8174 - val_mse: 3687397632.0000 - val_mae: 32789.8164\n",
      "Epoch 46/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 32912.9080 - mse: 3399113728.0000 - mae: 32912.9062 - val_loss: 32398.4873 - val_mse: 3582169600.0000 - val_mae: 32398.4883\n",
      "Epoch 47/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 32673.1415 - mse: 3339132416.0000 - mae: 32673.1426 - val_loss: 32063.1909 - val_mse: 3544704512.0000 - val_mae: 32063.1914\n",
      "Epoch 48/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 32366.5864 - mse: 3229170688.0000 - mae: 32366.5879 - val_loss: 31721.5335 - val_mse: 3423112448.0000 - val_mae: 31721.5352\n",
      "Epoch 49/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 32032.9609 - mse: 3162738688.0000 - mae: 32032.9629 - val_loss: 31399.2774 - val_mse: 3401724672.0000 - val_mae: 31399.2793\n",
      "Epoch 50/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 31766.1503 - mse: 3167131648.0000 - mae: 31766.1504 - val_loss: 31097.3084 - val_mse: 3390994944.0000 - val_mae: 31097.3086\n",
      "Epoch 51/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 31447.8609 - mse: 3094252800.0000 - mae: 31447.8613 - val_loss: 30854.7752 - val_mse: 3266578432.0000 - val_mae: 30854.7754\n",
      "Epoch 52/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 31189.9566 - mse: 3000397056.0000 - mae: 31189.9570 - val_loss: 30607.8105 - val_mse: 3206273024.0000 - val_mae: 30607.8105\n",
      "Epoch 53/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 30906.1672 - mse: 2964082688.0000 - mae: 30906.1660 - val_loss: 30301.4303 - val_mse: 3170702336.0000 - val_mae: 30301.4297\n",
      "Epoch 54/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 30733.3978 - mse: 2884133376.0000 - mae: 30733.4004 - val_loss: 30059.1189 - val_mse: 3100012800.0000 - val_mae: 30059.1191\n",
      "Epoch 55/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 30337.9737 - mse: 2869148160.0000 - mae: 30337.9746 - val_loss: 29736.8658 - val_mse: 3093284608.0000 - val_mae: 29736.8652\n",
      "Epoch 56/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 30110.4376 - mse: 2844614144.0000 - mae: 30110.4375 - val_loss: 29516.5865 - val_mse: 3032416000.0000 - val_mae: 29516.5859\n",
      "Epoch 57/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 29842.2553 - mse: 2755565824.0000 - mae: 29842.2578 - val_loss: 29306.7904 - val_mse: 2942079488.0000 - val_mae: 29306.7930\n",
      "Epoch 58/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 29582.8713 - mse: 2711717376.0000 - mae: 29582.8711 - val_loss: 29026.4334 - val_mse: 2921681920.0000 - val_mae: 29026.4316\n",
      "Epoch 59/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 29309.3289 - mse: 2683772160.0000 - mae: 29309.3281 - val_loss: 28784.3810 - val_mse: 2886845440.0000 - val_mae: 28784.3809\n",
      "Epoch 60/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 29082.3228 - mse: 2622009600.0000 - mae: 29082.3223 - val_loss: 28549.5197 - val_mse: 2815890688.0000 - val_mae: 28549.5215\n",
      "Epoch 61/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 28834.9966 - mse: 2594138624.0000 - mae: 28834.9980 - val_loss: 28312.5947 - val_mse: 2814755840.0000 - val_mae: 28312.5938\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 0s 40us/sample - loss: 28666.8111 - mse: 2538355456.0000 - mae: 28666.8145 - val_loss: 28097.4664 - val_mse: 2695168000.0000 - val_mae: 28097.4648\n",
      "Epoch 63/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 28322.4084 - mse: 2494517760.0000 - mae: 28322.4082 - val_loss: 27817.4965 - val_mse: 2710683392.0000 - val_mae: 27817.4980\n",
      "Epoch 64/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 28064.4826 - mse: 2473788160.0000 - mae: 28064.4824 - val_loss: 27579.1005 - val_mse: 2640122880.0000 - val_mae: 27579.0996\n",
      "Epoch 65/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 27809.1843 - mse: 2397599744.0000 - mae: 27809.1836 - val_loss: 27351.6093 - val_mse: 2574391808.0000 - val_mae: 27351.6094\n",
      "Epoch 66/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 27590.4498 - mse: 2378535936.0000 - mae: 27590.4512 - val_loss: 27088.6267 - val_mse: 2577430016.0000 - val_mae: 27088.6270\n",
      "Epoch 67/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 27332.7331 - mse: 2338890752.0000 - mae: 27332.7344 - val_loss: 26832.9916 - val_mse: 2505916672.0000 - val_mae: 26832.9902\n",
      "Epoch 68/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 27132.9241 - mse: 2280938240.0000 - mae: 27132.9238 - val_loss: 26599.3013 - val_mse: 2456101888.0000 - val_mae: 26599.3008\n",
      "Epoch 69/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 26861.6664 - mse: 2270328320.0000 - mae: 26861.6660 - val_loss: 26356.3519 - val_mse: 2464469248.0000 - val_mae: 26356.3516\n",
      "Epoch 70/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 26651.8437 - mse: 2221261824.0000 - mae: 26651.8418 - val_loss: 26178.0778 - val_mse: 2374016512.0000 - val_mae: 26178.0781\n",
      "Epoch 71/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 26406.8631 - mse: 2181677568.0000 - mae: 26406.8613 - val_loss: 25922.3242 - val_mse: 2356056576.0000 - val_mae: 25922.3242\n",
      "Epoch 72/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 26186.9732 - mse: 2165069312.0000 - mae: 26186.9707 - val_loss: 25710.9772 - val_mse: 2353251328.0000 - val_mae: 25710.9766\n",
      "Epoch 73/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 25972.6907 - mse: 2124021632.0000 - mae: 25972.6895 - val_loss: 25517.6851 - val_mse: 2278951680.0000 - val_mae: 25517.6855\n",
      "Epoch 74/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 25764.3247 - mse: 2094536832.0000 - mae: 25764.3242 - val_loss: 25304.9419 - val_mse: 2266011136.0000 - val_mae: 25304.9434\n",
      "Epoch 75/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 25545.5078 - mse: 2074092928.0000 - mae: 25545.5078 - val_loss: 25109.2382 - val_mse: 2232047616.0000 - val_mae: 25109.2383\n",
      "Epoch 76/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 25354.0839 - mse: 2038227968.0000 - mae: 25354.0820 - val_loss: 24918.8431 - val_mse: 2187702784.0000 - val_mae: 24918.8418\n",
      "Epoch 77/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 25242.9947 - mse: 2031783424.0000 - mae: 25242.9941 - val_loss: 24744.1317 - val_mse: 2174641408.0000 - val_mae: 24744.1328\n",
      "Epoch 78/200\n",
      "875/875 [==============================] - 0s 56us/sample - loss: 24973.9333 - mse: 1978150272.0000 - mae: 24973.9336 - val_loss: 24594.6866 - val_mse: 2095015296.0000 - val_mae: 24594.6875\n",
      "Epoch 79/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 24824.6571 - mse: 1944396672.0000 - mae: 24824.6562 - val_loss: 24366.5141 - val_mse: 2109148672.0000 - val_mae: 24366.5137\n",
      "Epoch 80/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 24594.3224 - mse: 1932884608.0000 - mae: 24594.3223 - val_loss: 24172.3483 - val_mse: 2050733184.0000 - val_mae: 24172.3496\n",
      "Epoch 81/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 24394.4345 - mse: 1896724608.0000 - mae: 24394.4316 - val_loss: 23984.9363 - val_mse: 2033422464.0000 - val_mae: 23984.9355\n",
      "Epoch 82/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 24203.3160 - mse: 1869453184.0000 - mae: 24203.3164 - val_loss: 23817.4332 - val_mse: 1981705600.0000 - val_mae: 23817.4336\n",
      "Epoch 83/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 23996.8241 - mse: 1840040064.0000 - mae: 23996.8223 - val_loss: 23650.3842 - val_mse: 1986067712.0000 - val_mae: 23650.3867\n",
      "Epoch 84/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 23816.4403 - mse: 1822111360.0000 - mae: 23816.4395 - val_loss: 23473.9195 - val_mse: 1933759616.0000 - val_mae: 23473.9180\n",
      "Epoch 85/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 23661.6733 - mse: 1797874304.0000 - mae: 23661.6738 - val_loss: 23297.8436 - val_mse: 1902402944.0000 - val_mae: 23297.8418\n",
      "Epoch 86/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 23464.1912 - mse: 1754165888.0000 - mae: 23464.1914 - val_loss: 23157.8007 - val_mse: 1845432576.0000 - val_mae: 23157.8027\n",
      "Epoch 87/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 23325.0028 - mse: 1753504384.0000 - mae: 23325.0020 - val_loss: 23000.9125 - val_mse: 1873480960.0000 - val_mae: 23000.9102\n",
      "Epoch 88/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 23078.2033 - mse: 1710056576.0000 - mae: 23078.2031 - val_loss: 22900.4742 - val_mse: 1766139520.0000 - val_mae: 22900.4746\n",
      "Epoch 89/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 22924.1984 - mse: 1683975424.0000 - mae: 22924.1973 - val_loss: 22685.4653 - val_mse: 1803777408.0000 - val_mae: 22685.4629\n",
      "Epoch 90/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 22735.6918 - mse: 1665989888.0000 - mae: 22735.6914 - val_loss: 22524.8484 - val_mse: 1726351360.0000 - val_mae: 22524.8477\n",
      "Epoch 91/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 22576.9503 - mse: 1634714112.0000 - mae: 22576.9512 - val_loss: 22370.5459 - val_mse: 1728747392.0000 - val_mae: 22370.5449\n",
      "Epoch 92/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 22412.5740 - mse: 1624562560.0000 - mae: 22412.5762 - val_loss: 22205.0930 - val_mse: 1675937664.0000 - val_mae: 22205.0918\n",
      "Epoch 93/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 22278.3144 - mse: 1612451840.0000 - mae: 22278.3145 - val_loss: 22084.9788 - val_mse: 1665667584.0000 - val_mae: 22084.9785\n",
      "Epoch 94/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 22147.5139 - mse: 1576895616.0000 - mae: 22147.5156 - val_loss: 22002.5922 - val_mse: 1655144448.0000 - val_mae: 22002.5918\n",
      "Epoch 95/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21970.4956 - mse: 1567874176.0000 - mae: 21970.4941 - val_loss: 21851.1985 - val_mse: 1614698368.0000 - val_mae: 21851.1992\n",
      "Epoch 96/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21872.9680 - mse: 1566191104.0000 - mae: 21872.9688 - val_loss: 21750.9215 - val_mse: 1598997888.0000 - val_mae: 21750.9219\n",
      "Epoch 97/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 21658.7359 - mse: 1527180928.0000 - mae: 21658.7344 - val_loss: 21631.3566 - val_mse: 1571045248.0000 - val_mae: 21631.3555\n",
      "Epoch 98/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21535.0744 - mse: 1512125696.0000 - mae: 21535.0762 - val_loss: 21531.9050 - val_mse: 1550869632.0000 - val_mae: 21531.9043\n",
      "Epoch 99/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21406.9039 - mse: 1498504064.0000 - mae: 21406.9023 - val_loss: 21477.9128 - val_mse: 1551986560.0000 - val_mae: 21477.9141\n",
      "Epoch 100/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21309.9070 - mse: 1495543808.0000 - mae: 21309.9062 - val_loss: 21326.4876 - val_mse: 1508244096.0000 - val_mae: 21326.4863\n",
      "Epoch 101/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21143.5757 - mse: 1467809920.0000 - mae: 21143.5762 - val_loss: 21270.7688 - val_mse: 1509671936.0000 - val_mae: 21270.7695\n",
      "Epoch 102/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 21097.3723 - mse: 1473245696.0000 - mae: 21097.3711 - val_loss: 21141.2468 - val_mse: 1461975168.0000 - val_mae: 21141.2461\n",
      "Epoch 103/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 20918.4408 - mse: 1444909312.0000 - mae: 20918.4414 - val_loss: 21055.9382 - val_mse: 1456916480.0000 - val_mae: 21055.9375\n",
      "Epoch 104/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 20792.5593 - mse: 1421545088.0000 - mae: 20792.5605 - val_loss: 20974.9823 - val_mse: 1436402304.0000 - val_mae: 20974.9844\n",
      "Epoch 105/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 20692.5485 - mse: 1424426496.0000 - mae: 20692.5488 - val_loss: 20877.6909 - val_mse: 1411402240.0000 - val_mae: 20877.6895\n",
      "Epoch 106/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 20604.2385 - mse: 1402346240.0000 - mae: 20604.2383 - val_loss: 20797.7569 - val_mse: 1414747008.0000 - val_mae: 20797.7559\n",
      "Epoch 107/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 20428.9411 - mse: 1392775296.0000 - mae: 20428.9395 - val_loss: 20691.8502 - val_mse: 1379262976.0000 - val_mae: 20691.8496\n",
      "Epoch 108/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 20310.7352 - mse: 1371703680.0000 - mae: 20310.7344 - val_loss: 20632.6425 - val_mse: 1386510848.0000 - val_mae: 20632.6406\n",
      "Epoch 109/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 20247.9204 - mse: 1375341312.0000 - mae: 20247.9180 - val_loss: 20503.6282 - val_mse: 1346466432.0000 - val_mae: 20503.6270\n",
      "Epoch 110/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 20150.9448 - mse: 1352233856.0000 - mae: 20150.9434 - val_loss: 20453.2196 - val_mse: 1352162432.0000 - val_mae: 20453.2188\n",
      "Epoch 111/200\n",
      "875/875 [==============================] - 0s 51us/sample - loss: 20008.7238 - mse: 1346882688.0000 - mae: 20008.7246 - val_loss: 20343.5764 - val_mse: 1322683904.0000 - val_mae: 20343.5762\n",
      "Epoch 112/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 19904.4428 - mse: 1329011200.0000 - mae: 19904.4434 - val_loss: 20271.7422 - val_mse: 1311042944.0000 - val_mae: 20271.7422\n",
      "Epoch 113/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 19856.4423 - mse: 1336356224.0000 - mae: 19856.4414 - val_loss: 20203.7542 - val_mse: 1294220928.0000 - val_mae: 20203.7559\n",
      "Epoch 114/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 19753.6719 - mse: 1305781120.0000 - mae: 19753.6738 - val_loss: 20174.8739 - val_mse: 1301983360.0000 - val_mae: 20174.8750\n",
      "Epoch 115/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 19645.1398 - mse: 1314793856.0000 - mae: 19645.1406 - val_loss: 20068.3879 - val_mse: 1256630912.0000 - val_mae: 20068.3887\n",
      "Epoch 116/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 19648.4559 - mse: 1284173312.0000 - mae: 19648.4570 - val_loss: 20057.2088 - val_mse: 1287119744.0000 - val_mae: 20057.2070\n",
      "Epoch 117/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 19591.5036 - mse: 1313691264.0000 - mae: 19591.5039 - val_loss: 19891.5525 - val_mse: 1239206912.0000 - val_mae: 19891.5527\n",
      "Epoch 118/200\n",
      "875/875 [==============================] - ETA: 0s - loss: 20356.2246 - mse: 1137640192.0000 - mae: 20356.22 - 0s 39us/sample - loss: 19403.6648 - mse: 1270871808.0000 - mae: 19403.6641 - val_loss: 19839.3858 - val_mse: 1241593088.0000 - val_mae: 19839.3867\n",
      "Epoch 119/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 19233.0874 - mse: 1271839360.0000 - mae: 19233.0879 - val_loss: 19770.6452 - val_mse: 1232100096.0000 - val_mae: 19770.6445\n",
      "Epoch 120/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 19133.3453 - mse: 1262985088.0000 - mae: 19133.3457 - val_loss: 19697.8521 - val_mse: 1210394368.0000 - val_mae: 19697.8516\n",
      "Epoch 121/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 19050.1847 - mse: 1250037632.0000 - mae: 19050.1855 - val_loss: 19689.0584 - val_mse: 1230125056.0000 - val_mae: 19689.0586\n",
      "Epoch 122/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 19116.7821 - mse: 1271860480.0000 - mae: 19116.7812 - val_loss: 19652.1954 - val_mse: 1190702464.0000 - val_mae: 19652.1934\n",
      "Epoch 123/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 19242.2720 - mse: 1243099136.0000 - mae: 19242.2734 - val_loss: 19666.1498 - val_mse: 1241775488.0000 - val_mae: 19666.1504\n",
      "Epoch 124/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 19099.0892 - mse: 1253461120.0000 - mae: 19099.0898 - val_loss: 19472.8076 - val_mse: 1164852736.0000 - val_mae: 19472.8086\n",
      "Epoch 125/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 18869.9694 - mse: 1230719232.0000 - mae: 18869.9688 - val_loss: 19366.6304 - val_mse: 1184779648.0000 - val_mae: 19366.6309\n",
      "Epoch 126/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 18740.1840 - mse: 1214035200.0000 - mae: 18740.1855 - val_loss: 19268.5169 - val_mse: 1160488320.0000 - val_mae: 19268.5156\n",
      "Epoch 127/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 18591.2873 - mse: 1214203392.0000 - mae: 18591.2871 - val_loss: 19217.5438 - val_mse: 1159788160.0000 - val_mae: 19217.5449\n",
      "Epoch 128/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 18518.2745 - mse: 1208263936.0000 - mae: 18518.2734 - val_loss: 19158.0329 - val_mse: 1147540480.0000 - val_mae: 19158.0312\n",
      "Epoch 129/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 18440.7062 - mse: 1205871488.0000 - mae: 18440.7070 - val_loss: 19110.1517 - val_mse: 1149812992.0000 - val_mae: 19110.1523\n",
      "Epoch 130/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 18355.9765 - mse: 1193727488.0000 - mae: 18355.9766 - val_loss: 19036.3234 - val_mse: 1138499712.0000 - val_mae: 19036.3242\n",
      "Epoch 131/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 18302.4886 - mse: 1193295488.0000 - mae: 18302.4883 - val_loss: 18988.3877 - val_mse: 1110444288.0000 - val_mae: 18988.3887\n",
      "Epoch 132/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 18261.4826 - mse: 1185553536.0000 - mae: 18261.4824 - val_loss: 18934.6058 - val_mse: 1129309952.0000 - val_mae: 18934.6055\n",
      "Epoch 133/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 18130.7367 - mse: 1177891456.0000 - mae: 18130.7363 - val_loss: 18847.1127 - val_mse: 1109801856.0000 - val_mae: 18847.1133\n",
      "Epoch 134/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 18066.0451 - mse: 1173158400.0000 - mae: 18066.0449 - val_loss: 18783.4999 - val_mse: 1097723264.0000 - val_mae: 18783.5000\n",
      "Epoch 135/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 17983.1522 - mse: 1163264896.0000 - mae: 17983.1523 - val_loss: 18756.8260 - val_mse: 1104585216.0000 - val_mae: 18756.8262\n",
      "Epoch 136/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17933.2847 - mse: 1164063360.0000 - mae: 17933.2832 - val_loss: 18680.9770 - val_mse: 1081000448.0000 - val_mae: 18680.9766\n",
      "Epoch 137/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 17887.2394 - mse: 1151747968.0000 - mae: 17887.2402 - val_loss: 18714.8559 - val_mse: 1099743872.0000 - val_mae: 18714.8555\n",
      "Epoch 138/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 17842.8075 - mse: 1156052096.0000 - mae: 17842.8066 - val_loss: 18601.6147 - val_mse: 1056974976.0000 - val_mae: 18601.6133\n",
      "Epoch 139/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17801.7181 - mse: 1150681344.0000 - mae: 17801.7188 - val_loss: 18665.0179 - val_mse: 1097522176.0000 - val_mae: 18665.0176\n",
      "Epoch 140/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 17748.6124 - mse: 1140249088.0000 - mae: 17748.6113 - val_loss: 18518.2631 - val_mse: 1048659072.0000 - val_mae: 18518.2617\n",
      "Epoch 141/200\n",
      "875/875 [==============================] - 0s 49us/sample - loss: 17758.8230 - mse: 1149724672.0000 - mae: 17758.8223 - val_loss: 18442.5755 - val_mse: 1060518336.0000 - val_mae: 18442.5762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 17687.2647 - mse: 1135186176.0000 - mae: 17687.2637 - val_loss: 18395.7437 - val_mse: 1062150912.0000 - val_mae: 18395.7441\n",
      "Epoch 143/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17609.4429 - mse: 1132757376.0000 - mae: 17609.4414 - val_loss: 18336.3470 - val_mse: 1027753280.0000 - val_mae: 18336.3477\n",
      "Epoch 144/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 17488.1520 - mse: 1122083712.0000 - mae: 17488.1504 - val_loss: 18327.8599 - val_mse: 1056812992.0000 - val_mae: 18327.8594\n",
      "Epoch 145/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17425.0753 - mse: 1116230528.0000 - mae: 17425.0723 - val_loss: 18204.6198 - val_mse: 1034436736.0000 - val_mae: 18204.6211\n",
      "Epoch 146/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17377.0743 - mse: 1122012288.0000 - mae: 17377.0762 - val_loss: 18163.8600 - val_mse: 1030784128.0000 - val_mae: 18163.8613\n",
      "Epoch 147/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 17396.2759 - mse: 1111387008.0000 - mae: 17396.2754 - val_loss: 18184.2555 - val_mse: 1041527872.0000 - val_mae: 18184.2539\n",
      "Epoch 148/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17260.2518 - mse: 1109857920.0000 - mae: 17260.2520 - val_loss: 18074.6257 - val_mse: 1020760320.0000 - val_mae: 18074.6250\n",
      "Epoch 149/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 17199.8291 - mse: 1109932416.0000 - mae: 17199.8281 - val_loss: 18049.5110 - val_mse: 1014392960.0000 - val_mae: 18049.5117\n",
      "Epoch 150/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 17182.4066 - mse: 1108845312.0000 - mae: 17182.4062 - val_loss: 18030.8170 - val_mse: 1015219328.0000 - val_mae: 18030.8164\n",
      "Epoch 151/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 17101.3466 - mse: 1096670464.0000 - mae: 17101.3457 - val_loss: 18051.7055 - val_mse: 1028786432.0000 - val_mae: 18051.7051\n",
      "Epoch 152/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 17055.0338 - mse: 1102136320.0000 - mae: 17055.0332 - val_loss: 17923.6182 - val_mse: 1001084928.0000 - val_mae: 17923.6191\n",
      "Epoch 153/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 17009.0473 - mse: 1095324800.0000 - mae: 17009.0488 - val_loss: 17910.2336 - val_mse: 1010014208.0000 - val_mae: 17910.2324\n",
      "Epoch 154/200\n",
      "875/875 [==============================] - 0s 49us/sample - loss: 16991.7414 - mse: 1086743552.0000 - mae: 16991.7422 - val_loss: 17949.5197 - val_mse: 1017308288.0000 - val_mae: 17949.5215\n",
      "Epoch 155/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 17096.0514 - mse: 1091735424.0000 - mae: 17096.0508 - val_loss: 17861.2997 - val_mse: 966968128.0000 - val_mae: 17861.3008\n",
      "Epoch 156/200\n",
      "875/875 [==============================] - 0s 50us/sample - loss: 16986.3134 - mse: 1079936384.0000 - mae: 16986.3145 - val_loss: 18168.3674 - val_mse: 1034316032.0000 - val_mae: 18168.3672\n",
      "Epoch 157/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 16974.8859 - mse: 1078603264.0000 - mae: 16974.8867 - val_loss: 17734.2783 - val_mse: 962257984.0000 - val_mae: 17734.2773\n",
      "Epoch 158/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 16888.1888 - mse: 1078302848.0000 - mae: 16888.1895 - val_loss: 17781.6946 - val_mse: 995831936.0000 - val_mae: 17781.6934\n",
      "Epoch 159/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 16776.9253 - mse: 1073562816.0000 - mae: 16776.9238 - val_loss: 17694.7598 - val_mse: 986110272.0000 - val_mae: 17694.7598\n",
      "Epoch 160/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 16717.6026 - mse: 1074370048.0000 - mae: 16717.6016 - val_loss: 17583.3909 - val_mse: 961468800.0000 - val_mae: 17583.3906\n",
      "Epoch 161/200\n",
      "875/875 [==============================] - 0s 52us/sample - loss: 16691.2477 - mse: 1068007040.0000 - mae: 16691.2480 - val_loss: 17671.0648 - val_mse: 985444800.0000 - val_mae: 17671.0645\n",
      "Epoch 162/200\n",
      "875/875 [==============================] - 0s 54us/sample - loss: 16598.7822 - mse: 1064255808.0000 - mae: 16598.7812 - val_loss: 17571.0806 - val_mse: 972678848.0000 - val_mae: 17571.0801\n",
      "Epoch 163/200\n",
      "875/875 [==============================] - 0s 55us/sample - loss: 16559.5344 - mse: 1063222016.0000 - mae: 16559.5352 - val_loss: 17467.9171 - val_mse: 955206528.0000 - val_mae: 17467.9160\n",
      "Epoch 164/200\n",
      "875/875 [==============================] - 0s 50us/sample - loss: 16492.8092 - mse: 1060057856.0000 - mae: 16492.8086 - val_loss: 17482.7971 - val_mse: 961699456.0000 - val_mae: 17482.7949\n",
      "Epoch 165/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 16490.9240 - mse: 1055152128.0000 - mae: 16490.9238 - val_loss: 17394.6478 - val_mse: 947386112.0000 - val_mae: 17394.6465\n",
      "Epoch 166/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 16429.0582 - mse: 1055754432.0000 - mae: 16429.0586 - val_loss: 17378.0499 - val_mse: 945150720.0000 - val_mae: 17378.0508\n",
      "Epoch 167/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 16403.3370 - mse: 1052347072.0000 - mae: 16403.3379 - val_loss: 17322.5169 - val_mse: 938634432.0000 - val_mae: 17322.5156\n",
      "Epoch 168/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 16308.0295 - mse: 1047697664.0000 - mae: 16308.0293 - val_loss: 17337.0666 - val_mse: 943325504.0000 - val_mae: 17337.0664\n",
      "Epoch 169/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 16260.4696 - mse: 1044720512.0000 - mae: 16260.4697 - val_loss: 17271.2394 - val_mse: 929111680.0000 - val_mae: 17271.2402\n",
      "Epoch 170/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 16285.0276 - mse: 1040234496.0000 - mae: 16285.0273 - val_loss: 17284.8497 - val_mse: 939781376.0000 - val_mae: 17284.8496\n",
      "Epoch 171/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 16207.4940 - mse: 1038036032.0000 - mae: 16207.4941 - val_loss: 17277.6617 - val_mse: 941023488.0000 - val_mae: 17277.6602\n",
      "Epoch 172/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 16225.3844 - mse: 1045734720.0000 - mae: 16225.3848 - val_loss: 17099.7018 - val_mse: 902326336.0000 - val_mae: 17099.7031\n",
      "Epoch 173/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 16284.6537 - mse: 1043115584.0000 - mae: 16284.6533 - val_loss: 17118.5862 - val_mse: 927099328.0000 - val_mae: 17118.5859\n",
      "Epoch 174/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 16095.9141 - mse: 1031944256.0000 - mae: 16095.9150 - val_loss: 17123.0501 - val_mse: 928202304.0000 - val_mae: 17123.0508\n",
      "Epoch 175/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 16048.6620 - mse: 1032617856.0000 - mae: 16048.6621 - val_loss: 17140.7078 - val_mse: 929632256.0000 - val_mae: 17140.7070\n",
      "Epoch 176/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 15976.8886 - mse: 1029538560.0000 - mae: 15976.8877 - val_loss: 17052.3783 - val_mse: 907575872.0000 - val_mae: 17052.3770\n",
      "Epoch 177/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 15979.1264 - mse: 1028710528.0000 - mae: 15979.1270 - val_loss: 17109.2596 - val_mse: 923234368.0000 - val_mae: 17109.2598\n",
      "Epoch 178/200\n",
      "875/875 [==============================] - 0s 38us/sample - loss: 15921.6212 - mse: 1023984064.0000 - mae: 15921.6201 - val_loss: 16987.4431 - val_mse: 907564544.0000 - val_mae: 16987.4434\n",
      "Epoch 179/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 15873.5562 - mse: 1021744192.0000 - mae: 15873.5557 - val_loss: 17005.4157 - val_mse: 911565760.0000 - val_mae: 17005.4160\n",
      "Epoch 180/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 15853.6659 - mse: 1023803136.0000 - mae: 15853.6660 - val_loss: 16959.7899 - val_mse: 901259456.0000 - val_mae: 16959.7891\n",
      "Epoch 181/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 15844.1550 - mse: 1022668096.0000 - mae: 15844.1562 - val_loss: 16916.9950 - val_mse: 896048576.0000 - val_mae: 16916.9961\n",
      "Epoch 182/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 15840.1166 - mse: 1021650304.0000 - mae: 15840.1162 - val_loss: 16995.5073 - val_mse: 913209152.0000 - val_mae: 16995.5078\n",
      "Epoch 183/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 15733.5927 - mse: 1013372096.0000 - mae: 15733.5928 - val_loss: 16942.6664 - val_mse: 907067456.0000 - val_mae: 16942.6660\n",
      "Epoch 184/200\n",
      "875/875 [==============================] - 0s 52us/sample - loss: 15757.4094 - mse: 1018625536.0000 - mae: 15757.4092 - val_loss: 16804.3087 - val_mse: 880907840.0000 - val_mae: 16804.3086\n",
      "Epoch 185/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 15813.8179 - mse: 1020756480.0000 - mae: 15813.8193 - val_loss: 16766.2529 - val_mse: 878199744.0000 - val_mae: 16766.2520\n",
      "Epoch 186/200\n",
      "875/875 [==============================] - 0s 51us/sample - loss: 15732.6624 - mse: 1012664128.0000 - mae: 15732.6621 - val_loss: 16807.2585 - val_mse: 891476736.0000 - val_mae: 16807.2578\n",
      "Epoch 187/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 15627.1304 - mse: 1006092544.0000 - mae: 15627.1299 - val_loss: 16800.0867 - val_mse: 891359744.0000 - val_mae: 16800.0859\n",
      "Epoch 188/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 15601.9975 - mse: 1004926848.0000 - mae: 15601.9980 - val_loss: 16765.4966 - val_mse: 886172224.0000 - val_mae: 16765.4980\n",
      "Epoch 189/200\n",
      "875/875 [==============================] - 0s 59us/sample - loss: 15549.4506 - mse: 1003031872.0000 - mae: 15549.4502 - val_loss: 16722.9239 - val_mse: 881489216.0000 - val_mae: 16722.9238\n",
      "Epoch 190/200\n",
      "875/875 [==============================] - 0s 51us/sample - loss: 15503.3710 - mse: 1002346752.0000 - mae: 15503.3711 - val_loss: 16687.4590 - val_mse: 877168896.0000 - val_mae: 16687.4590\n",
      "Epoch 191/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 15482.6329 - mse: 1003404864.0000 - mae: 15482.6328 - val_loss: 16665.7887 - val_mse: 874496512.0000 - val_mae: 16665.7891\n",
      "Epoch 192/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 15451.8608 - mse: 999483328.0000 - mae: 15451.8613 - val_loss: 16672.6327 - val_mse: 881610816.0000 - val_mae: 16672.6328\n",
      "Epoch 193/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 15434.8777 - mse: 997350336.0000 - mae: 15434.8799 - val_loss: 16641.1053 - val_mse: 877297344.0000 - val_mae: 16641.1055\n",
      "Epoch 194/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 15426.9045 - mse: 998390912.0000 - mae: 15426.9053 - val_loss: 16576.4123 - val_mse: 866146816.0000 - val_mae: 16576.4141\n",
      "Epoch 195/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 15363.0425 - mse: 994441472.0000 - mae: 15363.0420 - val_loss: 16535.5233 - val_mse: 858527488.0000 - val_mae: 16535.5234\n",
      "Epoch 196/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 15363.1406 - mse: 993133248.0000 - mae: 15363.1406 - val_loss: 16534.0337 - val_mse: 857782976.0000 - val_mae: 16534.0352\n",
      "Epoch 197/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 15345.4174 - mse: 989720640.0000 - mae: 15345.4180 - val_loss: 16509.9619 - val_mse: 857183424.0000 - val_mae: 16509.9609\n",
      "Epoch 198/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 15303.0392 - mse: 990336640.0000 - mae: 15303.0391 - val_loss: 16637.0989 - val_mse: 879153088.0000 - val_mae: 16637.0996\n",
      "Epoch 199/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 15276.3536 - mse: 988562880.0000 - mae: 15276.3535 - val_loss: 16534.3715 - val_mse: 867275264.0000 - val_mae: 16534.3730\n",
      "Epoch 200/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 15281.0355 - mse: 989321600.0000 - mae: 15281.0352 - val_loss: 16434.5742 - val_mse: 847250240.0000 - val_mae: 16434.5742\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_formatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 24;\n",
       "                var nbb_unformatted_code = \"df = pd.DataFrame(history.history)\\n\\ndf[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_formatted_code = \"df = pd.DataFrame(history.history)\\n\\ndf[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "\n",
    "df[[\"loss\", \"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUtnWyC60ls5"
   },
   "source": [
    "Finally, try your model using mean squared logarithmic error. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyTsb-4a0ls5"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nmodel = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nmodel = Sequential()\\n\\nmodel.add(Dense(128, input_dim=X.shape[1], activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(64, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(32, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(16, activation=\\\"relu\\\"))\\n\\nmodel.add(Dense(1, activation=\\\"linear\\\"))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(1, activation=\"linear\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"model.compile?\";\n",
       "                var nbb_formatted_code = \"model.compile?\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-NmgMGg0ls7"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"# Answer below:\\nmodel.compile(loss = 'mean_squared_logarithmic_error', optimizer='adam', metrics = ['mse', 'mae'])\";\n",
       "                var nbb_formatted_code = \"# Answer below:\\nmodel.compile(\\n    loss=\\\"mean_squared_logarithmic_error\\\", optimizer=\\\"adam\\\", metrics=[\\\"mse\\\", \\\"mae\\\"]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer below:\n",
    "model.compile(loss = 'mean_squared_logarithmic_error', optimizer='adam', metrics = ['mse', 'mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ParallelMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Train on 875 samples, validate on 219 samples\n",
      "Epoch 1/200\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027DDFAAAF78> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x0000027DDFAAAF78> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "Executing op __inference_initialize_variables_23569 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "Executing op __inference_distributed_function_23852 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "100/875 [==>...........................] - ETA: 9s - loss: 143.3568 - mse: 41396916224.0000 - mae: 189113.7031Executing op __inference_distributed_function_24016 in device /job:localhost/replica:0/task:0/device:GPU:0\n",
      "875/875 [==============================] - 2s 2ms/sample - loss: 132.0543 - mse: 41958342656.0000 - mae: 187499.1562 - val_loss: 118.6257 - val_mse: 41622745088.0000 - val_mae: 185166.1250\n",
      "Epoch 2/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 110.1038 - mse: 41957138432.0000 - mae: 187496.0156 - val_loss: 99.0440 - val_mse: 41620869120.0000 - val_mae: 185161.1562\n",
      "Epoch 3/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 92.4369 - mse: 41954492416.0000 - mae: 187489.0469 - val_loss: 83.7647 - val_mse: 41617104896.0000 - val_mae: 185151.1406\n",
      "Epoch 4/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 78.7962 - mse: 41949593600.0000 - mae: 187476.1719 - val_loss: 72.1791 - val_mse: 41610752000.0000 - val_mae: 185134.2500\n",
      "Epoch 5/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 68.4480 - mse: 41941897216.0000 - mae: 187455.8281 - val_loss: 63.3289 - val_mse: 41601347584.0000 - val_mae: 185109.2188\n",
      "Epoch 6/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 60.4750 - mse: 41931010048.0000 - mae: 187427.0156 - val_loss: 56.4100 - val_mse: 41588600832.0000 - val_mae: 185075.2812\n",
      "Epoch 7/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 54.1630 - mse: 41916612608.0000 - mae: 187388.9844 - val_loss: 50.8461 - val_mse: 41572261888.0000 - val_mae: 185031.7656\n",
      "Epoch 8/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 49.0268 - mse: 41898258432.0000 - mae: 187341.0156 - val_loss: 46.2507 - val_mse: 41552060416.0000 - val_mae: 184977.9219\n",
      "Epoch 9/200\n",
      "875/875 [==============================] - 0s 49us/sample - loss: 44.7469 - mse: 41876160512.0000 - mae: 187282.4688 - val_loss: 42.3658 - val_mse: 41527685120.0000 - val_mae: 184912.9375\n",
      "Epoch 10/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 41.0959 - mse: 41849491456.0000 - mae: 187212.1719 - val_loss: 39.0210 - val_mse: 41498816512.0000 - val_mae: 184835.9219\n",
      "Epoch 11/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 37.9339 - mse: 41818275840.0000 - mae: 187129.6875 - val_loss: 36.1022 - val_mse: 41465163776.0000 - val_mae: 184746.0938\n",
      "Epoch 12/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 35.1625 - mse: 41782108160.0000 - mae: 187034.0625 - val_loss: 33.5294 - val_mse: 41426448384.0000 - val_mae: 184642.7344\n",
      "Epoch 13/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 32.7103 - mse: 41740648448.0000 - mae: 186924.6094 - val_loss: 31.2441 - val_mse: 41382514688.0000 - val_mae: 184525.3750\n",
      "Epoch 14/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 30.5257 - mse: 41693745152.0000 - mae: 186800.8281 - val_loss: 29.2007 - val_mse: 41333198848.0000 - val_mae: 184393.5156\n",
      "Epoch 15/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 28.5687 - mse: 41641689088.0000 - mae: 186662.6562 - val_loss: 27.3612 - val_mse: 41278300160.0000 - val_mae: 184246.6250\n",
      "Epoch 16/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 26.7996 - mse: 41583652864.0000 - mae: 186508.7969 - val_loss: 25.6929 - val_mse: 41217548288.0000 - val_mae: 184083.9688\n",
      "Epoch 17/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 25.1878 - mse: 41518919680.0000 - mae: 186338.7969 - val_loss: 24.1685 - val_mse: 41150574592.0000 - val_mae: 183904.4219\n",
      "Epoch 18/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 23.7133 - mse: 41448505344.0000 - mae: 186151.2969 - val_loss: 22.7666 - val_mse: 41076948992.0000 - val_mae: 183706.8438\n",
      "Epoch 19/200\n",
      "875/875 [==============================] - 0s 52us/sample - loss: 22.3541 - mse: 41370611712.0000 - mae: 185945.5156 - val_loss: 21.4737 - val_mse: 40996478976.0000 - val_mae: 183490.6406\n",
      "Epoch 20/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 21.0991 - mse: 41287045120.0000 - mae: 185720.7344 - val_loss: 20.2721 - val_mse: 40908406784.0000 - val_mae: 183253.8750\n",
      "Epoch 21/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 19.9274 - mse: 41194336256.0000 - mae: 185474.0000 - val_loss: 19.1472 - val_mse: 40811945984.0000 - val_mae: 182994.2031\n",
      "Epoch 22/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 18.8271 - mse: 41092206592.0000 - mae: 185203.5625 - val_loss: 18.0916 - val_mse: 40706564096.0000 - val_mae: 182710.0781\n",
      "Epoch 23/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 17.7964 - mse: 40982020096.0000 - mae: 184908.5781 - val_loss: 17.1012 - val_mse: 40591912960.0000 - val_mae: 182400.6719\n",
      "Epoch 24/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 16.8295 - mse: 40862167040.0000 - mae: 184587.5938 - val_loss: 16.1739 - val_mse: 40468127744.0000 - val_mae: 182065.9219\n",
      "Epoch 25/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 15.9246 - mse: 40732340224.0000 - mae: 184241.8594 - val_loss: 15.3078 - val_mse: 40335425536.0000 - val_mae: 181706.3906\n",
      "Epoch 26/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 15.0790 - mse: 40593551360.0000 - mae: 183871.2344 - val_loss: 14.5000 - val_mse: 40194064384.0000 - val_mae: 181322.5781\n",
      "Epoch 27/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 14.2917 - mse: 40447401984.0000 - mae: 183476.0781 - val_loss: 13.7450 - val_mse: 40043876352.0000 - val_mae: 180914.0469\n",
      "Epoch 28/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 13.5556 - mse: 40291500032.0000 - mae: 183056.8906 - val_loss: 13.0404 - val_mse: 39885324288.0000 - val_mae: 180481.7188\n",
      "Epoch 29/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 12.8667 - mse: 40126087168.0000 - mae: 182614.7656 - val_loss: 12.3833 - val_mse: 39718887424.0000 - val_mae: 180026.6875\n",
      "Epoch 30/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 12.2258 - mse: 39955197952.0000 - mae: 182148.2031 - val_loss: 11.7673 - val_mse: 39543947264.0000 - val_mae: 179547.2188\n",
      "Epoch 31/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 11.6244 - mse: 39775207424.0000 - mae: 181658.7344 - val_loss: 11.1907 - val_mse: 39361114112.0000 - val_mae: 179044.8438\n",
      "Epoch 32/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 11.0605 - mse: 39585746944.0000 - mae: 181145.4062 - val_loss: 10.6509 - val_mse: 39170744320.0000 - val_mae: 178520.1875\n",
      "Epoch 33/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 10.5321 - mse: 39389368320.0000 - mae: 180610.6406 - val_loss: 10.1441 - val_mse: 38972755968.0000 - val_mae: 177972.9531\n",
      "Epoch 34/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 10.0363 - mse: 39185084416.0000 - mae: 180053.0781 - val_loss: 9.6677 - val_mse: 38767255552.0000 - val_mae: 177403.0625\n",
      "Epoch 35/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 9.5703 - mse: 38976237568.0000 - mae: 179472.9531 - val_loss: 9.2191 - val_mse: 38554210304.0000 - val_mae: 176810.5625\n",
      "Epoch 36/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 9.1302 - mse: 38756069376.0000 - mae: 178870.2656 - val_loss: 8.7970 - val_mse: 38334414848.0000 - val_mae: 176196.9062\n",
      "Epoch 37/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 8.7169 - mse: 38532476928.0000 - mae: 178247.0625 - val_loss: 8.3984 - val_mse: 38107209728.0000 - val_mae: 175560.7344\n",
      "Epoch 38/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 8.3257 - mse: 38298497024.0000 - mae: 177600.0781 - val_loss: 8.0226 - val_mse: 37873594368.0000 - val_mae: 174903.9062\n",
      "Epoch 39/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 7.9568 - mse: 38059634688.0000 - mae: 176934.6875 - val_loss: 7.6675 - val_mse: 37633359872.0000 - val_mae: 174225.9844\n",
      "Epoch 40/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 7.6082 - mse: 37814947840.0000 - mae: 176247.1094 - val_loss: 7.3314 - val_mse: 37386575872.0000 - val_mae: 173526.9062\n",
      "Epoch 41/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 7.2772 - mse: 37558956032.0000 - mae: 175538.4062 - val_loss: 7.0139 - val_mse: 37134233600.0000 - val_mae: 172808.9844\n",
      "Epoch 42/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 6.9650 - mse: 37301710848.0000 - mae: 174810.7812 - val_loss: 6.7124 - val_mse: 36875321344.0000 - val_mae: 172069.4062\n",
      "Epoch 43/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 6.6682 - mse: 37036072960.0000 - mae: 174061.4219 - val_loss: 6.4266 - val_mse: 36610654208.0000 - val_mae: 171310.0781\n",
      "Epoch 44/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 6.3868 - mse: 36765384704.0000 - mae: 173292.5469 - val_loss: 6.1551 - val_mse: 36340191232.0000 - val_mae: 170530.7969\n",
      "Epoch 45/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 6.1197 - mse: 36490874880.0000 - mae: 172504.4531 - val_loss: 5.8970 - val_mse: 36064022528.0000 - val_mae: 169731.4531\n",
      "Epoch 46/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 5.8656 - mse: 36208271360.0000 - mae: 171694.8438 - val_loss: 5.6517 - val_mse: 35782627328.0000 - val_mae: 168913.0625\n",
      "Epoch 47/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 5.6237 - mse: 35920752640.0000 - mae: 170867.8594 - val_loss: 5.4185 - val_mse: 35496386560.0000 - val_mae: 168076.6406\n",
      "Epoch 48/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 5.3936 - mse: 35628539904.0000 - mae: 170022.3125 - val_loss: 5.1966 - val_mse: 35205464064.0000 - val_mae: 167222.1562\n",
      "Epoch 49/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 5.1746 - mse: 35331821568.0000 - mae: 169159.9062 - val_loss: 4.9852 - val_mse: 34909925376.0000 - val_mae: 166349.7031\n",
      "Epoch 50/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 4.9659 - mse: 35028938752.0000 - mae: 168278.3438 - val_loss: 4.7837 - val_mse: 34609905664.0000 - val_mae: 165459.4219\n",
      "Epoch 51/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 4.7672 - mse: 34725314560.0000 - mae: 167379.4062 - val_loss: 4.5915 - val_mse: 34305456128.0000 - val_mae: 164551.3438\n",
      "Epoch 52/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 4.5771 - mse: 34413375488.0000 - mae: 166463.3594 - val_loss: 4.4083 - val_mse: 33997398016.0000 - val_mae: 163627.2344\n",
      "Epoch 53/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 4.3962 - mse: 34100903936.0000 - mae: 165531.8438 - val_loss: 4.2332 - val_mse: 33685178368.0000 - val_mae: 162685.6250\n",
      "Epoch 54/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 4.2228 - mse: 33779361792.0000 - mae: 164581.7344 - val_loss: 4.0665 - val_mse: 33370394624.0000 - val_mae: 161730.5312\n",
      "Epoch 55/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 4.0582 - mse: 33461653504.0000 - mae: 163618.0625 - val_loss: 3.9068 - val_mse: 33051488256.0000 - val_mae: 160757.3594\n",
      "Epoch 56/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 3.9001 - mse: 33136263168.0000 - mae: 162637.5312 - val_loss: 3.7543 - val_mse: 32729987072.0000 - val_mae: 159770.3125\n",
      "Epoch 57/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 3.7493 - mse: 32808503296.0000 - mae: 161643.3125 - val_loss: 3.6085 - val_mse: 32405409792.0000 - val_mae: 158767.7969\n",
      "Epoch 58/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 3.6049 - mse: 32478822400.0000 - mae: 160632.8750 - val_loss: 3.4690 - val_mse: 32078188544.0000 - val_mae: 157750.7344\n",
      "Epoch 59/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 3.4667 - mse: 32144996352.0000 - mae: 159607.7500 - val_loss: 3.3356 - val_mse: 31748841472.0000 - val_mae: 156720.5000\n",
      "Epoch 60/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 3.3348 - mse: 31811633152.0000 - mae: 158570.1094 - val_loss: 3.2077 - val_mse: 31416610816.0000 - val_mae: 155674.6719\n",
      "Epoch 61/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 3.2079 - mse: 31471806464.0000 - mae: 157518.4531 - val_loss: 3.0856 - val_mse: 31083534336.0000 - val_mae: 154618.9219\n",
      "Epoch 62/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 0s 41us/sample - loss: 3.0871 - mse: 31137228800.0000 - mae: 156454.7656 - val_loss: 2.9683 - val_mse: 30747662336.0000 - val_mae: 153547.4219\n",
      "Epoch 63/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.9708 - mse: 30793820160.0000 - mae: 155376.6719 - val_loss: 2.8561 - val_mse: 30410764288.0000 - val_mae: 152465.1094\n",
      "Epoch 64/200\n",
      "875/875 [==============================] - ETA: 0s - loss: 3.0465 - mse: 32908689408.0000 - mae: 162931.40 - 0s 40us/sample - loss: 2.8596 - mse: 30453198848.0000 - mae: 154288.8281 - val_loss: 2.7485 - val_mse: 30072311808.0000 - val_mae: 151370.3594\n",
      "Epoch 65/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 2.7526 - mse: 30106744832.0000 - mae: 153187.5625 - val_loss: 2.6456 - val_mse: 29733632000.0000 - val_mae: 150266.6875\n",
      "Epoch 66/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.6506 - mse: 29763999744.0000 - mae: 152077.1406 - val_loss: 2.5468 - val_mse: 29393287168.0000 - val_mae: 149149.8281\n",
      "Epoch 67/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 2.5526 - mse: 29420279808.0000 - mae: 150954.2188 - val_loss: 2.4518 - val_mse: 29051774976.0000 - val_mae: 148020.8906\n",
      "Epoch 68/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.4582 - mse: 29071996928.0000 - mae: 149821.3750 - val_loss: 2.3610 - val_mse: 28710977536.0000 - val_mae: 146885.5469\n",
      "Epoch 69/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 2.3680 - mse: 28726112256.0000 - mae: 148679.8281 - val_loss: 2.2739 - val_mse: 28370016256.0000 - val_mae: 145740.9375\n",
      "Epoch 70/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.2814 - mse: 28377968640.0000 - mae: 147530.1094 - val_loss: 2.1904 - val_mse: 28029224960.0000 - val_mae: 144588.1250\n",
      "Epoch 71/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.1983 - mse: 28034562048.0000 - mae: 146371.4531 - val_loss: 2.1102 - val_mse: 27688214528.0000 - val_mae: 143425.7500\n",
      "Epoch 72/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.1183 - mse: 27683409920.0000 - mae: 145202.6719 - val_loss: 2.0332 - val_mse: 27347703808.0000 - val_mae: 142255.5469\n",
      "Epoch 73/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 2.0420 - mse: 27341602816.0000 - mae: 144025.7812 - val_loss: 1.9588 - val_mse: 27005476864.0000 - val_mae: 141070.3438\n",
      "Epoch 74/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1.9681 - mse: 26997430272.0000 - mae: 142836.4844 - val_loss: 1.8876 - val_mse: 26665015296.0000 - val_mae: 139881.3125\n",
      "Epoch 75/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1.8971 - mse: 26649591808.0000 - mae: 141644.3594 - val_loss: 1.8192 - val_mse: 26325659648.0000 - val_mae: 138686.2344\n",
      "Epoch 76/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1.8290 - mse: 26304780288.0000 - mae: 140443.1719 - val_loss: 1.7536 - val_mse: 25987440640.0000 - val_mae: 137485.0625\n",
      "Epoch 77/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1.7636 - mse: 25960456192.0000 - mae: 139239.3750 - val_loss: 1.6906 - val_mse: 25650520064.0000 - val_mae: 136278.3750\n",
      "Epoch 78/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1.7008 - mse: 25619740672.0000 - mae: 138028.9062 - val_loss: 1.6299 - val_mse: 25314242560.0000 - val_mae: 135063.3438\n",
      "Epoch 79/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.6400 - mse: 25274421248.0000 - mae: 136815.5469 - val_loss: 1.5718 - val_mse: 24980897792.0000 - val_mae: 133848.1562\n",
      "Epoch 80/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1.5823 - mse: 24939657216.0000 - mae: 135598.3594 - val_loss: 1.5156 - val_mse: 24646873088.0000 - val_mae: 132620.0156\n",
      "Epoch 81/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.5264 - mse: 24602552320.0000 - mae: 134369.1094 - val_loss: 1.4616 - val_mse: 24315076608.0000 - val_mae: 131388.9844\n",
      "Epoch 82/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.4725 - mse: 24266491904.0000 - mae: 133139.9062 - val_loss: 1.4098 - val_mse: 23985664000.0000 - val_mae: 130155.3906\n",
      "Epoch 83/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 1.4210 - mse: 23934341120.0000 - mae: 131907.1094 - val_loss: 1.3600 - val_mse: 23657693184.0000 - val_mae: 128916.1562\n",
      "Epoch 84/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 1.3714 - mse: 23603968000.0000 - mae: 130672.0547 - val_loss: 1.3120 - val_mse: 23331911680.0000 - val_mae: 127673.6172\n",
      "Epoch 85/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.3233 - mse: 23270207488.0000 - mae: 129432.6875 - val_loss: 1.2661 - val_mse: 23010541568.0000 - val_mae: 126435.6719\n",
      "Epoch 86/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1.2775 - mse: 22942316544.0000 - mae: 128192.3438 - val_loss: 1.2219 - val_mse: 22690867200.0000 - val_mae: 125192.3438\n",
      "Epoch 87/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.2333 - mse: 22619531264.0000 - mae: 126951.4297 - val_loss: 1.1794 - val_mse: 22373715968.0000 - val_mae: 123947.3672\n",
      "Epoch 88/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 1.1907 - mse: 22295687168.0000 - mae: 125708.5078 - val_loss: 1.1386 - val_mse: 22059780096.0000 - val_mae: 122702.8828\n",
      "Epoch 89/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.1498 - mse: 21976385536.0000 - mae: 124463.3984 - val_loss: 1.0992 - val_mse: 21747462144.0000 - val_mae: 121452.5000\n",
      "Epoch 90/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 1.1104 - mse: 21661620224.0000 - mae: 123213.5234 - val_loss: 1.0613 - val_mse: 21437755392.0000 - val_mae: 120200.2734\n",
      "Epoch 91/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.0725 - mse: 21350371328.0000 - mae: 121964.0312 - val_loss: 1.0247 - val_mse: 21129709568.0000 - val_mae: 118942.5391\n",
      "Epoch 92/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 1.0358 - mse: 21035020288.0000 - mae: 120714.7031 - val_loss: 0.9896 - val_mse: 20826624000.0000 - val_mae: 117692.3672\n",
      "Epoch 93/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 1.0007 - mse: 20727812096.0000 - mae: 119471.5078 - val_loss: 0.9559 - val_mse: 20526573568.0000 - val_mae: 116452.1719\n",
      "Epoch 94/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.9670 - mse: 20424679424.0000 - mae: 118219.2734 - val_loss: 0.9234 - val_mse: 20229001216.0000 - val_mae: 115209.6172\n",
      "Epoch 95/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.9345 - mse: 20123521024.0000 - mae: 116976.2188 - val_loss: 0.8921 - val_mse: 19934605312.0000 - val_mae: 113967.3438\n",
      "Epoch 96/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.9030 - mse: 19824211968.0000 - mae: 115734.0703 - val_loss: 0.8621 - val_mse: 19645288448.0000 - val_mae: 112740.5938\n",
      "Epoch 97/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.8729 - mse: 19528423424.0000 - mae: 114497.5938 - val_loss: 0.8333 - val_mse: 19359229952.0000 - val_mae: 111516.9219\n",
      "Epoch 98/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.8441 - mse: 19243622400.0000 - mae: 113250.2969 - val_loss: 0.8053 - val_mse: 19074045952.0000 - val_mae: 110291.1719\n",
      "Epoch 99/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.8159 - mse: 18952112128.0000 - mae: 112018.2578 - val_loss: 0.7786 - val_mse: 18794745856.0000 - val_mae: 109078.5547\n",
      "Epoch 100/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.7892 - mse: 18669770752.0000 - mae: 110788.3516 - val_loss: 0.7527 - val_mse: 18517905408.0000 - val_mae: 107863.6016\n",
      "Epoch 101/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.7633 - mse: 18391654400.0000 - mae: 109559.5859 - val_loss: 0.7278 - val_mse: 18244075520.0000 - val_mae: 106648.5312\n",
      "Epoch 102/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.7383 - mse: 18111995904.0000 - mae: 108337.7188 - val_loss: 0.7040 - val_mse: 17975384064.0000 - val_mae: 105442.6172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.7143 - mse: 17838196736.0000 - mae: 107119.9844 - val_loss: 0.6812 - val_mse: 17711464448.0000 - val_mae: 104244.4766\n",
      "Epoch 104/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.6913 - mse: 17573273600.0000 - mae: 105906.3750 - val_loss: 0.6590 - val_mse: 17449785344.0000 - val_mae: 103043.4375\n",
      "Epoch 105/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.6691 - mse: 17307363328.0000 - mae: 104697.7578 - val_loss: 0.6377 - val_mse: 17191643136.0000 - val_mae: 101850.0547\n",
      "Epoch 106/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.6476 - mse: 17044239360.0000 - mae: 103493.2578 - val_loss: 0.6174 - val_mse: 16938815488.0000 - val_mae: 100672.9531\n",
      "Epoch 107/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.6271 - mse: 16788304896.0000 - mae: 102303.0938 - val_loss: 0.5976 - val_mse: 16687781888.0000 - val_mae: 99490.8516\n",
      "Epoch 108/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.6072 - mse: 16536043520.0000 - mae: 101121.8281 - val_loss: 0.5786 - val_mse: 16440585216.0000 - val_mae: 98313.1328\n",
      "Epoch 109/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.5880 - mse: 16282280960.0000 - mae: 99948.1719 - val_loss: 0.5604 - val_mse: 16199477248.0000 - val_mae: 97150.1484\n",
      "Epoch 110/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.5697 - mse: 16037531648.0000 - mae: 98784.7734 - val_loss: 0.5429 - val_mse: 15961372672.0000 - val_mae: 95988.0156\n",
      "Epoch 111/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.5520 - mse: 15797432320.0000 - mae: 97636.0312 - val_loss: 0.5260 - val_mse: 15727112192.0000 - val_mae: 94839.7812\n",
      "Epoch 112/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.5349 - mse: 15558553600.0000 - mae: 96501.0938 - val_loss: 0.5098 - val_mse: 15496826880.0000 - val_mae: 93706.5781\n",
      "Epoch 113/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.5186 - mse: 15326380032.0000 - mae: 95371.0703 - val_loss: 0.4929 - val_mse: 15251570688.0000 - val_mae: 92486.9141\n",
      "Epoch 114/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.4983 - mse: 15029710848.0000 - mae: 93939.1562 - val_loss: 0.4699 - val_mse: 14908802048.0000 - val_mae: 90750.9375\n",
      "Epoch 115/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.4734 - mse: 14654543872.0000 - mae: 92083.7500 - val_loss: 0.4446 - val_mse: 14517372928.0000 - val_mae: 88739.6562\n",
      "Epoch 116/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.4471 - mse: 14246630400.0000 - mae: 89998.0469 - val_loss: 0.4185 - val_mse: 14099528704.0000 - val_mae: 86576.0000\n",
      "Epoch 117/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.4205 - mse: 13813088256.0000 - mae: 87776.5391 - val_loss: 0.3928 - val_mse: 13669011456.0000 - val_mae: 84324.9141\n",
      "Epoch 118/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.3941 - mse: 13363679232.0000 - mae: 85483.9062 - val_loss: 0.3682 - val_mse: 13238583296.0000 - val_mae: 82038.4922\n",
      "Epoch 119/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.3693 - mse: 12926205952.0000 - mae: 83149.3281 - val_loss: 0.3448 - val_mse: 12810326016.0000 - val_mae: 79719.9062\n",
      "Epoch 120/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.3454 - mse: 12482129920.0000 - mae: 80801.8203 - val_loss: 0.3230 - val_mse: 12394658816.0000 - val_mae: 77410.0156\n",
      "Epoch 121/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.3238 - mse: 12071176192.0000 - mae: 78456.0469 - val_loss: 0.3026 - val_mse: 11987704832.0000 - val_mae: 75080.6875\n",
      "Epoch 122/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.3034 - mse: 11658249216.0000 - mae: 76169.3594 - val_loss: 0.2840 - val_mse: 11598597120.0000 - val_mae: 72870.5000\n",
      "Epoch 123/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.2845 - mse: 11259762688.0000 - mae: 73949.4844 - val_loss: 0.2671 - val_mse: 11229972480.0000 - val_mae: 70771.2578\n",
      "Epoch 124/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.2677 - mse: 10894527488.0000 - mae: 71823.7656 - val_loss: 0.2516 - val_mse: 10876871680.0000 - val_mae: 68748.2891\n",
      "Epoch 125/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.2522 - mse: 10541004800.0000 - mae: 69816.0625 - val_loss: 0.2377 - val_mse: 10545102848.0000 - val_mae: 66842.4609\n",
      "Epoch 126/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.2383 - mse: 10209631232.0000 - mae: 67912.1719 - val_loss: 0.2251 - val_mse: 10230897664.0000 - val_mae: 65124.5156\n",
      "Epoch 127/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 0.2257 - mse: 9892489216.0000 - mae: 66134.1953 - val_loss: 0.2139 - val_mse: 9940201472.0000 - val_mae: 63612.6055\n",
      "Epoch 128/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.2144 - mse: 9602194432.0000 - mae: 64449.2891 - val_loss: 0.2039 - val_mse: 9669015552.0000 - val_mae: 62188.6211\n",
      "Epoch 129/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 0.2041 - mse: 9326782464.0000 - mae: 62939.3242 - val_loss: 0.1949 - val_mse: 9416418304.0000 - val_mae: 60890.1406\n",
      "Epoch 130/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1952 - mse: 9076419584.0000 - mae: 61553.8242 - val_loss: 0.1868 - val_mse: 9177043968.0000 - val_mae: 59719.6016\n",
      "Epoch 131/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1869 - mse: 8831398912.0000 - mae: 60295.5703 - val_loss: 0.1796 - val_mse: 8957453312.0000 - val_mae: 58671.1680\n",
      "Epoch 132/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 0.1797 - mse: 8613951488.0000 - mae: 59141.5234 - val_loss: 0.1732 - val_mse: 8750981120.0000 - val_mae: 57703.8477\n",
      "Epoch 133/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1730 - mse: 8406050816.0000 - mae: 58072.4375 - val_loss: 0.1676 - val_mse: 8563848704.0000 - val_mae: 56881.4414\n",
      "Epoch 134/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1673 - mse: 8218419200.0000 - mae: 57208.2344 - val_loss: 0.1625 - val_mse: 8387633664.0000 - val_mae: 56130.6797\n",
      "Epoch 135/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1621 - mse: 8041519616.0000 - mae: 56405.4922 - val_loss: 0.1580 - val_mse: 8226103808.0000 - val_mae: 55531.7617\n",
      "Epoch 136/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 0.1575 - mse: 7879160832.0000 - mae: 55720.6914 - val_loss: 0.1541 - val_mse: 8077608448.0000 - val_mae: 55046.2500\n",
      "Epoch 137/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1534 - mse: 7728582656.0000 - mae: 55100.1250 - val_loss: 0.1507 - val_mse: 7940911104.0000 - val_mae: 54631.6133\n",
      "Epoch 138/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 0.1498 - mse: 7591634944.0000 - mae: 54577.3594 - val_loss: 0.1475 - val_mse: 7812071936.0000 - val_mae: 54262.0977\n",
      "Epoch 139/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1466 - mse: 7467366400.0000 - mae: 54098.1484 - val_loss: 0.1448 - val_mse: 7692973568.0000 - val_mae: 53966.9023\n",
      "Epoch 140/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1436 - mse: 7343285760.0000 - mae: 53666.8398 - val_loss: 0.1424 - val_mse: 7588724736.0000 - val_mae: 53720.0234\n",
      "Epoch 141/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 0.1411 - mse: 7235518976.0000 - mae: 53309.3203 - val_loss: 0.1404 - val_mse: 7494148096.0000 - val_mae: 53521.8359\n",
      "Epoch 142/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1390 - mse: 7141827584.0000 - mae: 53032.6367 - val_loss: 0.1385 - val_mse: 7402175488.0000 - val_mae: 53331.6602\n",
      "Epoch 143/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1369 - mse: 7047463936.0000 - mae: 52772.5742 - val_loss: 0.1370 - val_mse: 7320555008.0000 - val_mae: 53155.3242\n",
      "Epoch 144/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1352 - mse: 6967532032.0000 - mae: 52550.4805 - val_loss: 0.1355 - val_mse: 7243813888.0000 - val_mae: 52989.7891\n",
      "Epoch 145/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1336 - mse: 6887803392.0000 - mae: 52340.2344 - val_loss: 0.1343 - val_mse: 7175135232.0000 - val_mae: 52849.8906\n",
      "Epoch 146/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1322 - mse: 6816975872.0000 - mae: 52158.1992 - val_loss: 0.1332 - val_mse: 7111203840.0000 - val_mae: 52727.8047\n",
      "Epoch 147/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1310 - mse: 6752116224.0000 - mae: 51996.5547 - val_loss: 0.1322 - val_mse: 7050516480.0000 - val_mae: 52617.5508\n",
      "Epoch 148/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1298 - mse: 6688946176.0000 - mae: 51848.5820 - val_loss: 0.1314 - val_mse: 6997409280.0000 - val_mae: 52515.9961\n",
      "Epoch 149/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1290 - mse: 6636558336.0000 - mae: 51746.6016 - val_loss: 0.1306 - val_mse: 6942731264.0000 - val_mae: 52413.1133\n",
      "Epoch 150/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1280 - mse: 6579536384.0000 - mae: 51609.8242 - val_loss: 0.1299 - val_mse: 6896675840.0000 - val_mae: 52334.2383\n",
      "Epoch 151/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1272 - mse: 6533655552.0000 - mae: 51501.8047 - val_loss: 0.1293 - val_mse: 6854229504.0000 - val_mae: 52272.5312\n",
      "Epoch 152/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 0.1266 - mse: 6489729024.0000 - mae: 51409.5234 - val_loss: 0.1288 - val_mse: 6814620160.0000 - val_mae: 52225.0078\n",
      "Epoch 153/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1259 - mse: 6448303616.0000 - mae: 51329.1445 - val_loss: 0.1284 - val_mse: 6779607040.0000 - val_mae: 52186.7539\n",
      "Epoch 154/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 0.1254 - mse: 6412623872.0000 - mae: 51272.6484 - val_loss: 0.1280 - val_mse: 6744771584.0000 - val_mae: 52152.7852\n",
      "Epoch 155/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1249 - mse: 6377850880.0000 - mae: 51220.9336 - val_loss: 0.1276 - val_mse: 6711123456.0000 - val_mae: 52122.0547\n",
      "Epoch 156/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1245 - mse: 6344316928.0000 - mae: 51173.3945 - val_loss: 0.1272 - val_mse: 6681172480.0000 - val_mae: 52093.2617\n",
      "Epoch 157/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1240 - mse: 6313577472.0000 - mae: 51118.4141 - val_loss: 0.1269 - val_mse: 6655617024.0000 - val_mae: 52066.2812\n",
      "Epoch 158/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1236 - mse: 6286567936.0000 - mae: 51082.2734 - val_loss: 0.1267 - val_mse: 6632742912.0000 - val_mae: 52041.3984\n",
      "Epoch 159/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1234 - mse: 6262785024.0000 - mae: 51070.7383 - val_loss: 0.1264 - val_mse: 6606859776.0000 - val_mae: 52025.0820\n",
      "Epoch 160/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1230 - mse: 6236114432.0000 - mae: 51025.3867 - val_loss: 0.1262 - val_mse: 6586320896.0000 - val_mae: 52009.2227\n",
      "Epoch 161/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1227 - mse: 6214992384.0000 - mae: 50991.4883 - val_loss: 0.1260 - val_mse: 6567450624.0000 - val_mae: 51994.0273\n",
      "Epoch 162/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1224 - mse: 6195001344.0000 - mae: 50972.5938 - val_loss: 0.1258 - val_mse: 6547176960.0000 - val_mae: 51984.1797\n",
      "Epoch 163/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1222 - mse: 6174070784.0000 - mae: 50942.9961 - val_loss: 0.1256 - val_mse: 6529193472.0000 - val_mae: 51972.8125\n",
      "Epoch 164/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1219 - mse: 6155482112.0000 - mae: 50920.5000 - val_loss: 0.1254 - val_mse: 6511568896.0000 - val_mae: 51963.5859\n",
      "Epoch 165/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1217 - mse: 6137664512.0000 - mae: 50898.1289 - val_loss: 0.1252 - val_mse: 6496029696.0000 - val_mae: 51952.8711\n",
      "Epoch 166/200\n",
      "875/875 [==============================] - ETA: 0s - loss: 0.1405 - mse: 8079985664.0000 - mae: 55328.21 - 0s 41us/sample - loss: 0.1215 - mse: 6122150912.0000 - mae: 50868.6797 - val_loss: 0.1251 - val_mse: 6481757184.0000 - val_mae: 51938.9961\n",
      "Epoch 167/200\n",
      "875/875 [==============================] - 0s 46us/sample - loss: 0.1213 - mse: 6106632704.0000 - mae: 50850.4492 - val_loss: 0.1249 - val_mse: 6465966080.0000 - val_mae: 51928.5742\n",
      "Epoch 168/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1211 - mse: 6091286016.0000 - mae: 50828.1250 - val_loss: 0.1248 - val_mse: 6452418048.0000 - val_mae: 51914.2969\n",
      "Epoch 169/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1209 - mse: 6077858304.0000 - mae: 50805.5547 - val_loss: 0.1246 - val_mse: 6439831552.0000 - val_mae: 51896.6133\n",
      "Epoch 170/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1207 - mse: 6065726464.0000 - mae: 50776.1367 - val_loss: 0.1245 - val_mse: 6429893120.0000 - val_mae: 51873.9102\n",
      "Epoch 171/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 0.1205 - mse: 6054219776.0000 - mae: 50753.7422 - val_loss: 0.1243 - val_mse: 6417556992.0000 - val_mae: 51857.1680\n",
      "Epoch 172/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1204 - mse: 6041504768.0000 - mae: 50731.1055 - val_loss: 0.1242 - val_mse: 6405581312.0000 - val_mae: 51842.5625\n",
      "Epoch 173/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1202 - mse: 6030048256.0000 - mae: 50706.7070 - val_loss: 0.1240 - val_mse: 6395102208.0000 - val_mae: 51823.0469\n",
      "Epoch 174/200\n",
      "875/875 [==============================] - 0s 47us/sample - loss: 0.1200 - mse: 6019406336.0000 - mae: 50684.0625 - val_loss: 0.1239 - val_mse: 6384348160.0000 - val_mae: 51805.0547\n",
      "Epoch 175/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1199 - mse: 6009075200.0000 - mae: 50657.5898 - val_loss: 0.1237 - val_mse: 6375111168.0000 - val_mae: 51783.0469\n",
      "Epoch 176/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1197 - mse: 5999430144.0000 - mae: 50628.7461 - val_loss: 0.1236 - val_mse: 6366418432.0000 - val_mae: 51760.0469\n",
      "Epoch 177/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1195 - mse: 5990403072.0000 - mae: 50601.5898 - val_loss: 0.1234 - val_mse: 6357448192.0000 - val_mae: 51737.0703\n",
      "Epoch 178/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1194 - mse: 5980335616.0000 - mae: 50578.1758 - val_loss: 0.1233 - val_mse: 6346951680.0000 - val_mae: 51719.5781\n",
      "Epoch 179/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1192 - mse: 5970911744.0000 - mae: 50550.0352 - val_loss: 0.1231 - val_mse: 6337952256.0000 - val_mae: 51696.7344\n",
      "Epoch 180/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1191 - mse: 5961790976.0000 - mae: 50521.3242 - val_loss: 0.1230 - val_mse: 6329262080.0000 - val_mae: 51672.3477\n",
      "Epoch 181/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1189 - mse: 5953442304.0000 - mae: 50494.7578 - val_loss: 0.1228 - val_mse: 6321039360.0000 - val_mae: 51646.3750\n",
      "Epoch 182/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1187 - mse: 5945015296.0000 - mae: 50463.0586 - val_loss: 0.1227 - val_mse: 6312884224.0000 - val_mae: 51619.8750\n",
      "Epoch 183/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1186 - mse: 5936825344.0000 - mae: 50435.4531 - val_loss: 0.1225 - val_mse: 6304849920.0000 - val_mae: 51593.6484\n",
      "Epoch 184/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1184 - mse: 5929422848.0000 - mae: 50402.4219 - val_loss: 0.1224 - val_mse: 6298626048.0000 - val_mae: 51561.2656\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1183 - mse: 5922298368.0000 - mae: 50372.8398 - val_loss: 0.1222 - val_mse: 6290626560.0000 - val_mae: 51533.0195\n",
      "Epoch 186/200\n",
      "875/875 [==============================] - 0s 42us/sample - loss: 0.1181 - mse: 5914451456.0000 - mae: 50341.6055 - val_loss: 0.1220 - val_mse: 6281854464.0000 - val_mae: 51506.8359\n",
      "Epoch 187/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1180 - mse: 5906874368.0000 - mae: 50309.7188 - val_loss: 0.1219 - val_mse: 6275122688.0000 - val_mae: 51473.9766\n",
      "Epoch 188/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1178 - mse: 5899638272.0000 - mae: 50277.0312 - val_loss: 0.1217 - val_mse: 6267639808.0000 - val_mae: 51444.0703\n",
      "Epoch 189/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1176 - mse: 5893355520.0000 - mae: 50243.9805 - val_loss: 0.1216 - val_mse: 6262629888.0000 - val_mae: 51405.3242\n",
      "Epoch 190/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1175 - mse: 5886963200.0000 - mae: 50210.2070 - val_loss: 0.1214 - val_mse: 6254928384.0000 - val_mae: 51375.4492\n",
      "Epoch 191/200\n",
      "875/875 [==============================] - 0s 39us/sample - loss: 0.1173 - mse: 5878965760.0000 - mae: 50182.2812 - val_loss: 0.1212 - val_mse: 6246866432.0000 - val_mae: 51347.1445\n",
      "Epoch 192/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1172 - mse: 5872771072.0000 - mae: 50146.5781 - val_loss: 0.1211 - val_mse: 6241033728.0000 - val_mae: 51309.6133\n",
      "Epoch 193/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1170 - mse: 5865388032.0000 - mae: 50115.4023 - val_loss: 0.1209 - val_mse: 6232886784.0000 - val_mae: 51281.7266\n",
      "Epoch 194/200\n",
      "875/875 [==============================] - 0s 43us/sample - loss: 0.1168 - mse: 5859083776.0000 - mae: 50080.2422 - val_loss: 0.1208 - val_mse: 6227081728.0000 - val_mae: 51244.9297\n",
      "Epoch 195/200\n",
      "875/875 [==============================] - 0s 48us/sample - loss: 0.1167 - mse: 5852387840.0000 - mae: 50047.5273 - val_loss: 0.1206 - val_mse: 6219894272.0000 - val_mae: 51211.2266\n",
      "Epoch 196/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1165 - mse: 5845706752.0000 - mae: 50012.2578 - val_loss: 0.1204 - val_mse: 6214115328.0000 - val_mae: 51173.9922\n",
      "Epoch 197/200\n",
      "875/875 [==============================] - 0s 41us/sample - loss: 0.1164 - mse: 5841434624.0000 - mae: 49972.3828 - val_loss: 0.1203 - val_mse: 6209531904.0000 - val_mae: 51130.5977\n",
      "Epoch 198/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1162 - mse: 5834983936.0000 - mae: 49938.7383 - val_loss: 0.1201 - val_mse: 6201542656.0000 - val_mae: 51100.8281\n",
      "Epoch 199/200\n",
      "875/875 [==============================] - 0s 40us/sample - loss: 0.1161 - mse: 5827035648.0000 - mae: 49907.9023 - val_loss: 0.1199 - val_mse: 6193371136.0000 - val_mae: 51072.9258\n",
      "Epoch 200/200\n",
      "875/875 [==============================] - 0s 44us/sample - loss: 0.1159 - mse: 5820615168.0000 - mae: 49870.2031 - val_loss: 0.1198 - val_mse: 6188859904.0000 - val_mae: 51028.7305\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_formatted_code = \"history = model.fit(\\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"df = pd.DataFrame(history.history)\";\n",
       "                var nbb_formatted_code = \"df = pd.DataFrame(history.history)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"df[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_formatted_code = \"df[[\\\"loss\\\", \\\"val_loss\\\"]].plot()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[[\"loss\", \"val_loss\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 82 Lecture 2 Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
