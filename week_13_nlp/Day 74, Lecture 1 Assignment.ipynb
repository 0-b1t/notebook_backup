{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F4I54bZzMImI"
   },
   "source": [
    "# Machine Learning: Text Classification Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBhHdzKtMImK"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"from nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom sklearn.pipeline import Pipeline\\nfrom nltk.stem.snowball import SnowballStemmer\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import cross_val_score, train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\\n\\nimport requests\\nfrom bs4 import BeautifulSoup\";\n",
       "                var nbb_formatted_code = \"from nltk import word_tokenize\\nfrom nltk.corpus import stopwords\\nfrom sklearn.pipeline import Pipeline\\nfrom nltk.stem.snowball import SnowballStemmer\\nfrom nltk.stem.wordnet import WordNetLemmatizer\\nfrom sklearn.metrics import classification_report\\nfrom sklearn.model_selection import cross_val_score, train_test_split\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\\nfrom sklearn.model_selection import train_test_split as tts\\nfrom nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\\n\\nimport requests\\nfrom bs4 import BeautifulSoup\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d2tVDJJaMImN"
   },
   "source": [
    "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REtoZb_iMImO"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"PATH = \\\"corpata/AP_News\\\"\\n\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\nCAT_PATTERN = r\\\"([\\\\w_\\\\s]+)\\\"\\n\\ncorpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)\";\n",
       "                var nbb_formatted_code = \"PATH = \\\"corpata/AP_News\\\"\\n\\nDOC_PATTERN = r\\\".*\\\\.txt\\\"\\nCAT_PATTERN = r\\\"([\\\\w_\\\\s]+)\\\"\\n\\ncorpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATH = \"corpata/AP_News\"\n",
    "\n",
    "DOC_PATTERN = r\".*\\.txt\"\n",
    "CAT_PATTERN = r\"([\\w_\\s]+)\"\n",
    "\n",
    "corpus = CategorizedPlaintextCorpusReader(PATH, DOC_PATTERN, cat_pattern=CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SQMuBvquMImP"
   },
   "source": [
    "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8jU4ZNM-MImQ"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\\n\\n# categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\\ncategories = [fileid.split(\\\"/\\\")[0] for fileid in corpus.fileids()]\";\n",
       "                var nbb_formatted_code = \"docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\\n\\n# categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\\ncategories = [fileid.split(\\\"/\\\")[0] for fileid in corpus.fileids()]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\n",
    "\n",
    "# categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]\n",
    "categories = [fileid.split(\"/\")[0] for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGRABGW8MImR"
   },
   "source": [
    "### Preprocess the corpus, ensuring to include the following steps.\n",
    "\n",
    "- Word tokenize the documents.\n",
    "- Lemmatize, stem, and lowercase all tokens.\n",
    "- Remove punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hk-Nlze1MImS"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def preprocess(docs):\\n    lemmatizer = WordNetLemmatizer()\\n    stemmer = SnowballStemmer(\\\"english\\\")\\n    preprocessed = []\\n\\n    for doc in docs:\\n        tokenized = word_tokenize(doc)\\n\\n        cleaned = [\\n            stemmer.stem(lemmatizer.lemmatize(token.lower()))\\n            for token in tokenized\\n            if not token.lower() in stopwords.words(\\\"english\\\")\\n            if token.isalpha()\\n        ]\\n        untokenized = \\\" \\\".join(cleaned)\\n        preprocessed.append(untokenized)\\n\\n    return preprocessed\";\n",
       "                var nbb_formatted_code = \"def preprocess(docs):\\n    lemmatizer = WordNetLemmatizer()\\n    stemmer = SnowballStemmer(\\\"english\\\")\\n    preprocessed = []\\n\\n    for doc in docs:\\n        tokenized = word_tokenize(doc)\\n\\n        cleaned = [\\n            stemmer.stem(lemmatizer.lemmatize(token.lower()))\\n            for token in tokenized\\n            if not token.lower() in stopwords.words(\\\"english\\\")\\n            if token.isalpha()\\n        ]\\n        untokenized = \\\" \\\".join(cleaned)\\n        preprocessed.append(untokenized)\\n\\n    return preprocessed\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(docs):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    preprocessed = []\n",
    "\n",
    "    for doc in docs:\n",
    "        tokenized = word_tokenize(doc)\n",
    "\n",
    "        cleaned = [\n",
    "            stemmer.stem(lemmatizer.lemmatize(token.lower()))\n",
    "            for token in tokenized\n",
    "            if not token.lower() in stopwords.words(\"english\")\n",
    "            if token.isalpha()\n",
    "        ]\n",
    "        untokenized = \" \".join(cleaned)\n",
    "        preprocessed.append(untokenized)\n",
    "\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"preprocessed = preprocess(docs)\";\n",
       "                var nbb_formatted_code = \"preprocessed = preprocess(docs)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed = preprocess(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'honolulu ap univers hawaii seek addit fund student mental health servic scholarship item new supplementari budget request offici said board regent approv fiscal year supplement oper budget million thursday honolulu report request submit state legislatur democrat gov univers request million hire psychologist system univers hawaii manoa eight psychologist hilo campus three west oahu campus posit communiti colleg one posit said allyson tanouy coordin mental health throughout univers system nation standard one mental health profession per student tanouy said add posit one per low mental health fund would also expand program prevent suicid reduc mental health stigma provid peer educ alert new student parent colleg transit challeng offici said largest item supplement budget million expand hawaii promis program scholarship state institut univers propos flat amount cover tuition fee hawaii resid qualifi feder pell grant look focus needi student go campus said donald straney vice presid academ plan polici would add hawaii promis top pell grant cover total tuition cost student receiv grant request also cover staf increas includ posit oper mainten secur educ cultur program student mentor inform honolulu http'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"preprocessed[0]\";\n",
       "                var nbb_formatted_code = \"preprocessed[0]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gZE-q4ziMImT"
   },
   "source": [
    "### Split the data into training and testing sets with the size of the test set being 30% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hKFyBgjBMImU"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"X_train, X_test, y_train, y_test = train_test_split(\\n    preprocessed, categories, test_size=0.3, random_state=42\\n)\";\n",
       "                var nbb_formatted_code = \"X_train, X_test, y_train, y_test = train_test_split(\\n    preprocessed, categories, test_size=0.3, random_state=42\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    preprocessed, categories, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reegQu_5MImV"
   },
   "source": [
    "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3JJ3hjNMImW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\n",
       "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                        class_weight=None, criterion='gini',\n",
       "                                        max_depth=None, max_features='auto',\n",
       "                                        max_leaf_nodes=None, max_samples=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=100, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"model = Pipeline(\\n    [\\n        #         (\\\"vect\\\", CountVectorizer()),\\n        #         (\\\"tfidf\\\", TfidfTransformer()),\\n        (\\\"tfidf\\\", TfidfVectorizer()),\\n        (\\\"rfc\\\", RandomForestClassifier()),\\n    ]\\n)\\n\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_formatted_code = \"model = Pipeline(\\n    [\\n        #         (\\\"vect\\\", CountVectorizer()),\\n        #         (\\\"tfidf\\\", TfidfTransformer()),\\n        (\\\"tfidf\\\", TfidfVectorizer()),\\n        (\\\"rfc\\\", RandomForestClassifier()),\\n    ]\\n)\\n\\nmodel.fit(X_train, y_train)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        #         (\"vect\", CountVectorizer()),\n",
    "        #         (\"tfidf\", TfidfTransformer()),\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"rfc\", RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8484848484848485"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"model.score(X_test, y_test)\";\n",
       "                var nbb_formatted_code = \"model.score(X_test, y_test)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFt6djjpMImX"
   },
   "source": [
    "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWT99tvHMImY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.69      0.92      0.79        12\n",
      "    politics       0.90      0.90      0.90        20\n",
      "      sports       0.95      0.95      0.95        20\n",
      "        tech       0.80      0.57      0.67        14\n",
      "\n",
      "    accuracy                           0.85        66\n",
      "   macro avg       0.83      0.83      0.83        66\n",
      "weighted avg       0.86      0.85      0.84        66\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"y_pred = model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\";\n",
       "                var nbb_formatted_code = \"y_pred = model.predict(X_test)\\nprint(classification_report(y_test, y_pred))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2BwKeBNMImZ"
   },
   "source": [
    "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n9cTvc6MIma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807902097902098"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"scores = cross_val_score(model, preprocessed, categories, cv=10, scoring=\\\"f1_macro\\\")\\n\\nscores.mean()\";\n",
       "                var nbb_formatted_code = \"scores = cross_val_score(model, preprocessed, categories, cv=10, scoring=\\\"f1_macro\\\")\\n\\nscores.mean()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = cross_val_score(model, preprocessed, categories, cv=10, scoring=\"f1_macro\")\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1H9ERShiMImb"
   },
   "source": [
    "### Ingest, preprocess, and predict the topic of the article at the following URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7piaPcKNMImd",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"url = \\\"https://www.nytimes.com/2019/11/25/business/uber-london.html\\\"\\n\\nresponse = requests.get(url)\\ncontent = response.text\\n\\nsoup = BeautifulSoup(content)\\n\\ncleaned_lines = [\\n    line.text for line in soup.find_all(\\\"p\\\", class_=\\\"css-158dogj evys1bk0\\\")\\n]\";\n",
       "                var nbb_formatted_code = \"url = \\\"https://www.nytimes.com/2019/11/25/business/uber-london.html\\\"\\n\\nresponse = requests.get(url)\\ncontent = response.text\\n\\nsoup = BeautifulSoup(content)\\n\\ncleaned_lines = [\\n    line.text for line in soup.find_all(\\\"p\\\", class_=\\\"css-158dogj evys1bk0\\\")\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://www.nytimes.com/2019/11/25/business/uber-london.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "cleaned_lines = [\n",
    "    line.text for line in soup.find_all(\"p\", class_=\"css-158dogj evys1bk0\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech'], dtype='<U8')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"doc = \\\" \\\".join(cleaned_lines)\\nprocessed = preprocess([doc])\\n\\nmodel.predict(processed)\";\n",
       "                var nbb_formatted_code = \"doc = \\\" \\\".join(cleaned_lines)\\nprocessed = preprocess([doc])\\n\\nmodel.predict(processed)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = \" \".join(cleaned_lines)\n",
    "processed = preprocess([doc])\n",
    "\n",
    "model.predict(processed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 74, Lecture 1: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
