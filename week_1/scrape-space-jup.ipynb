{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                title               authors  \\\n",
      "0           Pictures from space! Our image of the day     [Space.com Staff]   \n",
      "1   On This Day in Space! July 24, 1950: First roc...   [Hanneke Weitering]   \n",
      "2   The curtain is about to come down on Comet NEO...             [Joe Rao]   \n",
      "3   NASA Jupiter probe images huge moon Ganymede l...           [Mike Wall]   \n",
      "4   'Star Trek' showrunner Alex Kurtzman talks abo...       [Scott Snowden]   \n",
      "5   Watch live @ 11:30 pm ET: Live views of Comet ...     [Space.com Staff]   \n",
      "6   Going nuclear: NASA's Perseverance Mars rover ...      [Meghan Bartels]   \n",
      "7   Space Force unveils its official logo and mott...        [Chelsea Gohd]   \n",
      "8   Meet 'Star Trek: Prodigy:' New animated series...        [Chelsea Gohd]   \n",
      "9   Join Space.com in our 'Summer of Mars' webinar...         [Tariq Malik]   \n",
      "10    A Space Fan's Guide to San Diego Comic-Con 2020       [Scott Snowden]   \n",
      "11  Russia has tested an anti-satellite weapon in ...        [Chelsea Gohd]   \n",
      "12  Russia launches robotic cargo ship on zippy sp...      [Meghan Bartels]   \n",
      "13  China launches ambitious Tianwen-1 Mars rover ...           [Mike Wall]   \n",
      "14  NASA's Mars rover Perseverance is ready for it...           [Mike Wall]   \n",
      "15  SpaceX's next astronaut launch for NASA now ta...           [Mike Wall]   \n",
      "16  How did Earth crack? New study may explain ori...        [Chelsea Gohd]   \n",
      "17  Comet NEOWISE is making its closest approach t...   [Hanneke Weitering]   \n",
      "18  Watch Russia launch a new cargo ship to the In...         [Tariq Malik]   \n",
      "19  'Star Trek' fans rejoice! Holodeck spotted in ...        [Chelsea Gohd]   \n",
      "0   Satellites uncover widespread illegal fishing ...           [Mike Wall]   \n",
      "1                    Why Persevere â€” to Mars? (Op-Ed)          [Janet Ivey]   \n",
      "2   Multiplanet system around sunlike star photogr...           [Mike Wall]   \n",
      "3   Mistaken identity? Researchers uncover true na...        [Chelsea Gohd]   \n",
      "4   China's Tianwen-1 Mars rover mission gets a bo...       [Leonard David]   \n",
      "5   Homebound NASA scientist grows radishes to pra...        [Chelsea Gohd]   \n",
      "6   How to spot the bright Comet NEOWISE using mob...       [Chris Vaughan]   \n",
      "7   Book excerpt: 'How to Die in Space' on the bea...         [Paul Sutter]   \n",
      "8   'How to Die in Space' explores the dangers of ...      [Meghan Bartels]   \n",
      "9   Asteroid shower rained space rocks on Earth an...     [Charles Q. Choi]   \n",
      "10  Watch SpaceX catch an entire rocket nose cone ...           [Mike Wall]   \n",
      "11  NASA astronauts tie spacewalk record while pre...  [Robert Z. Pearlman]   \n",
      "12  China's first Mars rover Tianwen-1 launches th...        [Andrew Jones]   \n",
      "13  For Mars, Hope (and a little Perseverance, too...      [Meghan Bartels]   \n",
      "14  Watch two astronauts take a milestone spacewal...    [Elizabeth Howell]   \n",
      "15  New monument shines light on Houston suburb's ...  [Robert Z. Pearlman]   \n",
      "16  Apollo 11 spacecraft joysticks top $780K at au...  [Robert Z. Pearlman]   \n",
      "17  SpaceX launches South Korea's 1st military sat...        [Amy Thompson]   \n",
      "18      Venus probably has active volcanoes right now           [Mike Wall]   \n",
      "19  Comet NEOWISE shines over SpaceX Falcon 9 rock...           [Mike Wall]   \n",
      "\n",
      "                                             synopsis             date_time  \n",
      "0   This striking photo showcases both comet NEOWI...  2020-07-24T13:14:57Z  \n",
      "1   On July 24, 1950, the very first rocket to eve...  2020-07-24T11:39:18Z  \n",
      "2   The brightest comet to appear in Northern Hemi...  2020-07-24T11:36:42Z  \n",
      "3   NASA's Juno Jupiter probe has captured unprece...  2020-07-24T11:36:26Z  \n",
      "4   \"There's a difference between debate and toxic...  2020-07-24T11:35:48Z  \n",
      "5   The Lowell Observatory's webcast begins at 11:...  2020-07-23T22:16:17Z  \n",
      "6   NASA's Perseverance Mars rover is ready to rol...  2020-07-23T21:48:31Z  \n",
      "7   The U.S. Space Force unveiled its official mot...  2020-07-23T20:59:23Z  \n",
      "8   \"Star Trek: Prodigy,\" the all-new animated ser...  2020-07-23T20:58:21Z  \n",
      "9   Join Space.com for our free \"Summer of Mars\" w...  2020-07-23T16:06:41Z  \n",
      "10  We've scanned the schedule and pulled out the ...  2020-07-23T15:51:51Z  \n",
      "11  The U.S. Space Command announced that they hav...  2020-07-23T15:09:23Z  \n",
      "12  Five astronauts living in space are preparing ...  2020-07-23T14:52:17Z  \n",
      "13  The Tianwen-1 mission launched atop a Long Mar...  2020-07-23T11:50:59Z  \n",
      "14       The flight readiness review is in the books.  2020-07-23T11:34:49Z  \n",
      "15  Crew-1, which will send four astronauts to the...  2020-07-23T11:31:02Z  \n",
      "16  In a new study, scientists investigated the or...  2020-07-23T11:00:28Z  \n",
      "17                                                     2020-07-22T21:55:03Z  \n",
      "18  A Soyuz rocket will launch a Russian cargo shi...  2020-07-22T21:42:25Z  \n",
      "19             The Holodeck has beamed down to Earth.  2020-07-22T21:24:04Z  \n",
      "0   Orbital observations revealed extensive illega...  2020-07-22T18:56:38Z  \n",
      "1   In the year that is 2020, #WhyPersevere? Janet...  2020-07-22T18:52:26Z  \n",
      "2   For the first time ever, astronomers have dire...  2020-07-22T13:36:12Z  \n",
      "3   Over two dozen misidentified supermassive blac...  2020-07-22T12:05:14Z  \n",
      "4   China's bid to explore Mars involves several o...  2020-07-22T12:03:55Z  \n",
      "5   A NASA scientist is growing radishes at home t...  2020-07-22T12:03:12Z  \n",
      "6   Skywatchers the world over are buzzing about C...  2020-07-21T18:49:14Z  \n",
      "7   Sure, space looks pretty, but just because it ...  2020-07-21T18:48:26Z  \n",
      "8   Life on Earth can seem pretty hazardous, but i...  2020-07-21T18:47:00Z  \n",
      "9   An asteroid shower thought to have struck the ...  2020-07-21T18:43:21Z  \n",
      "10  SpaceX managed to snag both halves of a Falcon...  2020-07-21T18:41:23Z  \n",
      "11  Two NASA astronauts tied a spacewalk record wh...  2020-07-21T18:07:39Z  \n",
      "12  China's Mars rover will likely attempt to land...  2020-07-21T12:29:43Z  \n",
      "13  The United Arab Emirates is on its way to Mars...  2020-07-21T12:19:00Z  \n",
      "14  NASA astronauts Chris Cassidy and Bob Behnken ...  2020-07-21T12:11:49Z  \n",
      "15  A Houston suburb is celebrating its 50-year co...  2020-07-20T22:22:19Z  \n",
      "16  Three joysticks used to fly the historic space...  2020-07-20T22:22:10Z  \n",
      "17  SpaceX successfully launched South Korea's fir...  2020-07-20T21:51:06Z  \n",
      "18  Researchers have identified 37 features on Ven...  2020-07-20T20:26:20Z  \n",
      "19  The gorgeous image shows Comet NEOWISE blazing...  2020-07-20T20:11:45Z  \n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"import urllib.robotparser\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\n\\n\\nBASE_URL = 'https://www.space.com/news/'\\nROBOTS_TXT_URL = 'https://www.space.com/robots.txt'\\n\\nARTICLE_CARD_SELECTOR = 'article'\\nAUTHOR_SELECTOR = '.by-author span'\\nTITLE_SELECTOR = '.article-name'\\nSYNOPSIS_SELECTOR = '.synopsis'\\nDATETIME_SELECTOR = 'time'\\n\\n\\ndef get_space_news(page=1):\\n    page_url = BASE_URL + str(page)\\n\\n    robot_parser = urllib.robotparser.RobotFileParser()\\n    robot_parser.set_url(ROBOTS_TXT_URL)\\n    robot_parser.read()\\n\\n    # Check permission\\n    if not robot_parser.can_fetch(useragent='*', url=page_url):\\n        raise PermissionError('page disallowed for user-agent')\\n\\n    response = requests.get(page_url)\\n    html_text = response.text\\n\\n    soup = BeautifulSoup(html_text, 'html.parser')\\n    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n\\n    articles = []\\n    for card in article_cards:\\n        title = card.select(TITLE_SELECTOR)[0].text.strip()\\n        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\\n        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\\n        date_time = card.select(DATETIME_SELECTOR)[0]['datetime']\\n\\n        article = {\\n            'title': title,\\n            'authors': authors,\\n            'synopsis': synopsis,\\n            'date_time': date_time,\\n        }\\n\\n        articles.append(article)\\n\\n    return pd.DataFrame(articles)\\n\\n\\nif __name__ == '__main__':\\n    # Get new home page\\n    page_1_article_df = get_space_news()\\n    page_2_article_df = get_space_news(page=2)\\n\\n    article_df = pd.concat((page_1_article_df, page_2_article_df))\\n    print(article_df)\";\n",
       "                var nbb_formatted_code = \"import urllib.robotparser\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\n\\n\\nBASE_URL = \\\"https://www.space.com/news/\\\"\\nROBOTS_TXT_URL = \\\"https://www.space.com/robots.txt\\\"\\n\\nARTICLE_CARD_SELECTOR = \\\"article\\\"\\nAUTHOR_SELECTOR = \\\".by-author span\\\"\\nTITLE_SELECTOR = \\\".article-name\\\"\\nSYNOPSIS_SELECTOR = \\\".synopsis\\\"\\nDATETIME_SELECTOR = \\\"time\\\"\\n\\n\\ndef get_space_news(page=1):\\n    page_url = BASE_URL + str(page)\\n\\n    robot_parser = urllib.robotparser.RobotFileParser()\\n    robot_parser.set_url(ROBOTS_TXT_URL)\\n    robot_parser.read()\\n\\n    # Check permission\\n    if not robot_parser.can_fetch(useragent=\\\"*\\\", url=page_url):\\n        raise PermissionError(\\\"page disallowed for user-agent\\\")\\n\\n    response = requests.get(page_url)\\n    html_text = response.text\\n\\n    soup = BeautifulSoup(html_text, \\\"html.parser\\\")\\n    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n\\n    articles = []\\n    for card in article_cards:\\n        title = card.select(TITLE_SELECTOR)[0].text.strip()\\n        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\\n        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\\n        date_time = card.select(DATETIME_SELECTOR)[0][\\\"datetime\\\"]\\n\\n        article = {\\n            \\\"title\\\": title,\\n            \\\"authors\\\": authors,\\n            \\\"synopsis\\\": synopsis,\\n            \\\"date_time\\\": date_time,\\n        }\\n\\n        articles.append(article)\\n\\n    return pd.DataFrame(articles)\\n\\n\\nif __name__ == \\\"__main__\\\":\\n    # Get new home page\\n    page_1_article_df = get_space_news()\\n    page_2_article_df = get_space_news(page=2)\\n\\n    article_df = pd.concat((page_1_article_df, page_2_article_df))\\n    print(article_df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BASE_URL = 'https://www.space.com/news/'\n",
    "ROBOTS_TXT_URL = 'https://www.space.com/robots.txt'\n",
    "\n",
    "ARTICLE_CARD_SELECTOR = 'article'\n",
    "AUTHOR_SELECTOR = '.by-author span'\n",
    "TITLE_SELECTOR = '.article-name'\n",
    "SYNOPSIS_SELECTOR = '.synopsis'\n",
    "DATETIME_SELECTOR = 'time'\n",
    "\n",
    "\n",
    "def get_space_news(page=1):\n",
    "    page_url = BASE_URL + str(page)\n",
    "\n",
    "    robot_parser = urllib.robotparser.RobotFileParser()\n",
    "    robot_parser.set_url(ROBOTS_TXT_URL)\n",
    "    robot_parser.read()\n",
    "\n",
    "    # Check permission\n",
    "    if not robot_parser.can_fetch(useragent='*', url=page_url):\n",
    "        raise PermissionError('page disallowed for user-agent')\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    html_text = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\n",
    "\n",
    "    articles = []\n",
    "    for card in article_cards:\n",
    "        title = card.select(TITLE_SELECTOR)[0].text.strip()\n",
    "        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\n",
    "        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\n",
    "        date_time = card.select(DATETIME_SELECTOR)[0]['datetime']\n",
    "\n",
    "        article = {\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'synopsis': synopsis,\n",
    "            'date_time': date_time,\n",
    "        }\n",
    "\n",
    "        articles.append(article)\n",
    "\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get new home page\n",
    "    page_1_article_df = get_space_news()\n",
    "    page_2_article_df = get_space_news(page=2)\n",
    "\n",
    "    article_df = pd.concat((page_1_article_df, page_2_article_df))\n",
    "    print(article_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"import urllib.robotparser\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\n\\n\\nBASE_URL = 'https://www.space.com/news/'\\nROBOTS_TXT_URL = 'https://www.space.com/robots.txt'\\n\\nARTICLE_CARD_SELECTOR = 'article'\\nAUTHOR_SELECTOR = '.by-author span'\\nTITLE_SELECTOR = '.article-name'\\nSYNOPSIS_SELECTOR = '.synopsis'\\nDATETIME_SELECTOR = 'time'\";\n",
       "                var nbb_formatted_code = \"import urllib.robotparser\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport pandas as pd\\n\\n\\nBASE_URL = \\\"https://www.space.com/news/\\\"\\nROBOTS_TXT_URL = \\\"https://www.space.com/robots.txt\\\"\\n\\nARTICLE_CARD_SELECTOR = \\\"article\\\"\\nAUTHOR_SELECTOR = \\\".by-author span\\\"\\nTITLE_SELECTOR = \\\".article-name\\\"\\nSYNOPSIS_SELECTOR = \\\".synopsis\\\"\\nDATETIME_SELECTOR = \\\"time\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import urllib.robotparser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "BASE_URL = \"https://www.space.com/news/\"\n",
    "ROBOTS_TXT_URL = \"https://www.space.com/robots.txt\"\n",
    "\n",
    "ARTICLE_CARD_SELECTOR = \"article\"\n",
    "AUTHOR_SELECTOR = \".by-author span\"\n",
    "TITLE_SELECTOR = \".article-name\"\n",
    "SYNOPSIS_SELECTOR = \".synopsis\"\n",
    "DATETIME_SELECTOR = \"time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"\\nn_pages = 10\\nfor page in range(1, n_pages+1):\\n    page_url = BASE_URL + str(page)\\n\\n    response = requests.get(page_url)\\n\\n    if not response.ok:\\n        ValueError('invalid request')\\n\\n    html_text = response.text\\n    soup = BeautifulSoup(html_text, 'html.parser')\\n\\n    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n    #article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n\\n    articles = []\\n    #card = article_cards[0]\\n    for card in article_cards:\\n        title = card.select_one(TITLE_SELECTOR).text.strip()\\n        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\\n        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\\n        date_time = card.select(DATETIME_SELECTOR)[0]['datetime']\\n\\n        article = {\\n            'title': title,\\n            'authors': authors,\\n            'synopsis': synopsis,\\n            'date_time': date_time,\\n        }\\n\\n        articles.append(article)\\npd.DataFrame(articles)\";\n",
       "                var nbb_formatted_code = \"n_pages = 10\\nfor page in range(1, n_pages + 1):\\n    page_url = BASE_URL + str(page)\\n\\n    response = requests.get(page_url)\\n\\n    if not response.ok:\\n        ValueError(\\\"invalid request\\\")\\n\\n    html_text = response.text\\n    soup = BeautifulSoup(html_text, \\\"html.parser\\\")\\n\\n    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n    # article_cards = soup.select(ARTICLE_CARD_SELECTOR)\\n\\n    articles = []\\n    # card = article_cards[0]\\n    for card in article_cards:\\n        title = card.select_one(TITLE_SELECTOR).text.strip()\\n        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\\n        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\\n        date_time = card.select(DATETIME_SELECTOR)[0][\\\"datetime\\\"]\\n\\n        article = {\\n            \\\"title\\\": title,\\n            \\\"authors\\\": authors,\\n            \\\"synopsis\\\": synopsis,\\n            \\\"date_time\\\": date_time,\\n        }\\n\\n        articles.append(article)\\npd.DataFrame(articles)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_pages = 10\n",
    "for page in range(1, n_pages + 1):\n",
    "    page_url = BASE_URL + str(page)\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    if not response.ok:\n",
    "        ValueError(\"invalid request\")\n",
    "\n",
    "    html_text = response.text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    article_cards = soup.select(ARTICLE_CARD_SELECTOR)\n",
    "    # article_cards = soup.select(ARTICLE_CARD_SELECTOR)\n",
    "\n",
    "    articles = []\n",
    "    # card = article_cards[0]\n",
    "    for card in article_cards:\n",
    "        title = card.select_one(TITLE_SELECTOR).text.strip()\n",
    "        authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\n",
    "        synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\n",
    "        date_time = card.select(DATETIME_SELECTOR)[0][\"datetime\"]\n",
    "\n",
    "        article = {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"synopsis\": synopsis,\n",
    "            \"date_time\": date_time,\n",
    "        }\n",
    "\n",
    "        articles.append(article)\n",
    "pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cards = soup.select(ARTICLE_CARD_SELECTOR)\n",
    "\n",
    "articles = []\n",
    "card = article_cards[0]\n",
    "\n",
    "title = card.select_one(TITLE_SELECTOR).text.strip()\n",
    "authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\n",
    "synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\n",
    "date_time = card.select(DATETIME_SELECTOR)[0]['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = {\n",
    "    'title': title,\n",
    "    'authors': authors,\n",
    "    'synopsis': synopsis,\n",
    "    'date_time': date_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-07-24T13:14:57Z'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card.select(DATETIME_SELECTOR)[0]['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = card.select_one(TITLE_SELECTOR).text.strip()\n",
    "authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\n",
    "synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\n",
    "date_time = card.select(DATETIME_SELECTOR)[0]['datetime']\n",
    "\n",
    "\n",
    "article = {\n",
    "    'title': title,\n",
    "    'authors': authors,\n",
    "    'synopsis': synopsis,\n",
    "    'date_time': date_time,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articles.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(articles).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "article_cards = soup.select(ARTICLE_CARD_SELECTOR)\n",
    "\n",
    "articles = []\n",
    "for card in article_cards:\n",
    "    title = card.select_one(TITLE_SELECTOR).text.strip()\n",
    "    authors = [x.text.strip() for x in card.select(AUTHOR_SELECTOR)]\n",
    "    synopsis = card.select(SYNOPSIS_SELECTOR)[0].text.strip()\n",
    "    date_time = card.select(DATETIME_SELECTOR)[0]['datetime']\n",
    "\n",
    "    article = {\n",
    "        'title': title,\n",
    "        'authors': authors,\n",
    "        'synopsis': synopsis,\n",
    "        'date_time': date_time,\n",
    "    }\n",
    "\n",
    "    articles.append(article)\n",
    "\n",
    "return pd.DataFrame(articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
